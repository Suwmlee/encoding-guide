<!DOCTYPE HTML>
<html lang="zh" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>高级编码指南</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">介绍</a></li><li class="chapter-item expanded "><a href="filtering/filtering.html"><strong aria-hidden="true">1.</strong> 过滤Filtering</a></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">1.1.</strong> 载入视频Loading the Video</div></li><li class="chapter-item expanded "><a href="filtering/cropping.html"><strong aria-hidden="true">1.2.</strong> 裁剪Cropping</a></li><li class="chapter-item expanded "><a href="filtering/resizing.html"><strong aria-hidden="true">1.3.</strong> 调整大小(Resizing)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="filtering/descaling.html"><strong aria-hidden="true">1.3.1.</strong> 解放缩(descaling)与重新缩放(rescaling)</a></li><li class="chapter-item expanded "><a href="filtering/chroma_res.html"><strong aria-hidden="true">1.3.2.</strong> 色度重采样与偏移(Chroma Resampling and Shifting)</a></li></ol></li><li class="chapter-item expanded "><a href="filtering/bit_depths.html"><strong aria-hidden="true">1.4.</strong> 位深(Bit Depths)与抖动算法(Dither Algorithms)</a></li><li class="chapter-item expanded "><a href="filtering/debanding.html"><strong aria-hidden="true">1.5.</strong> 解带(Debanding)与解块(Deblocking)</a></li><li class="chapter-item expanded "><a href="filtering/graining.html"><strong aria-hidden="true">1.6.</strong> 粒化(Graining)</a></li><li class="chapter-item expanded "><a href="filtering/dirty_lines.html"><strong aria-hidden="true">1.7.</strong> 脏线(Dirty Lines)与边界问题(Border Issues)</a></li><li class="chapter-item expanded "><a href="filtering/detinting.html"><strong aria-hidden="true">1.8.</strong> 除着色(Detinting)与水平调整(Level Adjustment)</a></li><li class="chapter-item expanded "><a href="filtering/masking.html"><strong aria-hidden="true">1.9.</strong> 蒙版(Masking)</a></li><li class="chapter-item expanded "><a href="filtering/anti-aliasing.html"><strong aria-hidden="true">1.10.</strong> 抗锯齿(Anti-Aliasing)</a></li><li class="chapter-item expanded "><a href="filtering/deringing.html"><strong aria-hidden="true">1.11.</strong> 消除振铃(Deringing)</a></li><li class="chapter-item expanded "><a href="filtering/dehaloing.html"><strong aria-hidden="true">1.12.</strong> 去晕(Dehaloing)</a></li><li class="chapter-item expanded "><a href="filtering/denoising.html"><strong aria-hidden="true">1.13.</strong> 降噪(Denoising)</a></li><li class="chapter-item expanded "><a href="filtering/dehardsubbing.html"><strong aria-hidden="true">1.14.</strong> 剔除硬字幕(Dehardsubbing)与去台标(Delogoing)</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">2.</strong> 编码Encoding</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="encoding/testing.html"><strong aria-hidden="true">2.1.</strong> 测试编码(Test Encodes)</a></li><li class="chapter-item expanded "><a href="encoding/x264.html"><strong aria-hidden="true">2.2.</strong> x264 Settings</a></li><li class="chapter-item expanded "><a href="encoding/x265.html"><strong aria-hidden="true">2.3.</strong> x265 Settings</a></li><li class="chapter-item expanded "><a href="encoding/screenshots.html"><strong aria-hidden="true">2.4.</strong> 截图和对比(Screenshots and Comparisons)</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> 音频Audio</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">3.1.</strong> SoX: 抖动和向下混合(Dithering and Down-Mixing)</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.2.</strong> 单声道和立体声(Mono and Stereo)</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.3.</strong> 环绕声(Surround Sound)</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> 附录Appendix</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">4.1.</strong> 线性光处理(Linear Light Processing)</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.2.</strong> 使用FT寻找最佳分辨率</div></li><li class="chapter-item expanded "><a href="appendix/grain_matching.html"><strong aria-hidden="true">4.3.</strong> 颗粒匹配(Grain Matching)</a></li><li class="chapter-item expanded "><a href="appendix/gray.html"><strong aria-hidden="true">4.4.</strong> 黑白剪辑(Black &amp; White Clips)</a></li></ol></li><li class="chapter-item expanded "><a href="scriptorium.html"><strong aria-hidden="true">5.</strong> Scriptorium</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">高级编码指南</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="介绍"><a class="header" href="#介绍">介绍</a></h1>
<p><a href="https://git.concertos.live/Encode_Guide/mdbook-guide">https://git.concertos.live/Encode_Guide/mdbook-guide</a></p>
<p>本指南既可以作为对制作高质量编码作品感兴趣的新手的起点，也可以作为有压制经验人员的参考。
因此，在大多数功能介绍完并解释了它们的用途之后，可以找到深入的解释资料。
这些只是为想深入研究的人提供简单扩展，不是只依靠阅读就能运用的。</p>
<h1 id="术语"><a class="header" href="#术语">术语</a></h1>
<p>为此，涉及有关视频的基本术语和知识。
一些解释将被大大简化，因为在 Wikipedia、AviSynth wiki 等网站上应该有很多页面对这些主题进行解释。</p>
<h2 id="视频"><a class="header" href="#视频">视频</a></h2>
<p>消费类视频产品通常存储在 YCbCr 中，该术语通常与 YUV 互换使用。在本指南中，我们将主要使用术语 YUV，因为 VapourSynth 格式编写也是 YUV。</p>
<p>YUV 格式的内容将信息分为三个平面: Y, 指代 luma, 表示亮度, U 和 V，分别表示色度平面。
这些色度平面代表颜色之间的偏移，其中平面的中间值是中性点。</p>
<p>这个色度信息通常是二次采样的, 这意味着它存储在比亮度平面更低的帧大小中。
几乎所有消费者视频都采用 4:2:0 格式，这意味着色度平面是亮度平面大小的一半。
<a href="https://en.wikipedia.org/wiki/Chroma_subsampling">关于色度子采样的Wikipedia</a> 应该足以解释这是如何工作的。
由于我们通常希望保持 4:2:0 格式，因此我们的框架尺寸受到限制，因为亮度平面的大小必须被 2 整除。
这意味着我们不能进行不均匀的裁剪或将大小调整为不均匀的分辨率。
但是，在必要时，我们可以在每个平面中单独处理，这些将会在 <a href="">过滤章节</a> 进行解释。</p>
<p>此外，我们的信息必须以特定的精度存储。
通常，我们处理每平面 8-bit 的精度。
但是，对于超高清蓝光，每平面标准是 10-bit 的精度。
这意味着每个平面的可能值范围从 0 到 \(2^8 - 1 = 255\)。
在 <a href="filtering/bit_depths.html">位深章节</a>, 我们将介绍在更高的位深精度下工作，以提高过滤过程中的精度</p>
<h2 id="vapoursynth"><a class="header" href="#vapoursynth">VapourSynth</a></h2>
<p>我们将通过 Python 使用 VapourSynth 框架来进行裁剪、移除不需要的黑色边框、调整大小和消除源中不需要的产物。
虽然使用 Python 可能听起来很吓人，但那些没有经验的人不必担心，因为我们只会做非常基本的事情。</p>
<p>关于 VapourSynth 配置的资料不计其数，例如 <a href="https://guide.encode.moe/encoding/preparation.html#the-frameserver">the Irrational Encoding Wizardry's guide</a> 和 <a href="http://www.vapoursynth.com/doc/index.html">VapourSynth documentation</a>。
因此，本指南不会涵盖安装和配置。</p>
<p>在开始编写脚本前，知道每个clip/filter都必须有一个变量名是非常重要的:</p>
<pre><code class="language-py">clip_a = source(a)
clip_b = source(b)

filter_a = filter(clip_a)
filter_b = filter(clip_b)

filter_x_on_a = filter_x(clip_a)
filter_y_on_a = filter_y(clip_b)
</code></pre>
<p>此外，许多方法都在脚本模块或类似的集合中。
这些必须手动加载，然后在给定的别名下找到：</p>
<pre><code class="language-py">import vapoursynth as vs
core = vs.core
import awsmfunc as awf
import kagefunc as kgf
from vsutil import *

bbmod = awf.bbmod(...)
grain = kgf.adaptive_grain(...)
deband = core.f3kdb.Deband(...)

change_depth = depth(...)
</code></pre>
<p>为避免方法名冲突，通常不建议这样做 <code>from x import *</code>。</p>
<p>虽然此类模块中有许多filters，但也有一些filters可用作插件。
这些插件可以通过 <code>core.namespace.plugin</code> 或 替代调用 <code>clip.namespace.plugin</code>。
这意味着以下两个是等效的：</p>
<pre><code class="language-py">via_core = core.std.Crop(clip, ...)
via_clip = clip.std.Crop(...)
</code></pre>
<p>脚本里无法直接使用这些方法，这意味着以下是不可能的</p>
<pre><code class="language-py">not_possible = clip.awf.bbmod(...)
</code></pre>
<p>在本指南中，我们将会对要使用的源进行命名并设置变量名为 <code>src</code> 以方便之后反复对其操作。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><p>在处理视频的过程中，通常会遇到一些视觉上的问题，如带状物、变暗的边框等。
由于这些东西在视觉上并不讨人喜欢，而且大多数情况下并不是原创作者的本意，所以最好使用视频过滤器来修复它们。</p>
<h2 id="场景过滤"><a class="header" href="#场景过滤">场景过滤</a></h2>
<p>由于每个过滤器都有一定的破坏性，所以最好只在必要时应用它们。这通常是通过<code>ReplaceFramesSimple</code>完成的，它在<a href="https://github.com/Irrational-Encoding-Wizardry/Vapoursynth-RemapFrames"><code>RemapFrames</code></a>插件中。
<code>RemapFramesSimple</code>也可以用<code>Rfs</code>调用。
另外，可以使用Python解决方案，例如std.Trim和addition。
然而，<code>RemapFrames</code>往往更快，特别是对于较大的替换映射集。</p>
<p>让我们看一个对100到200帧和500到750帧应用<a href="filtering/filtering/debanding##neo_f3kdb"><code>f3kdb</code></a>解带过滤的例子:</p>
<pre><code class="language-py">src = core.ffms2.Source(&quot;video.mkv&quot;)
deband = source.neo_f3kdb.Deband(src)

replaced = core.remap.Rfs(src, deband, mappings=&quot;[100 200] [500 750]&quot;)
</code></pre>
<p>在插件和Python方法里有各种封装好的库，特别是前者的<a href="https://git.concertos.live/AHD/awsmfunc"><code>awsmfunc.rfs</code></a>和后者的<a href="https://lvsfunc.encode.moe/en/latest/#lvsfunc.util.replace_ranges"><code>lvsfunc.util.replace_frames</code></a> 。</p>
<h2 id="过滤顺序"><a class="header" href="#过滤顺序">过滤顺序</a></h2>
<p>为了让滤镜正常工作，不至于产生反效果，按正确的顺序应用它们是很重要的。
这一点对于像debanders和graners这样的滤镜尤其重要，因为把它们放在调整大小之前会完全否定它们的效果。</p>
<p>一般可接受的顺序是:</p>
<ol>
<li>载入视频 Load the source</li>
<li>裁剪 Crop</li>
<li>提高位深 Raise bit depth</li>
<li>除着色 Detint</li>
<li>修复脏线 Fix dirty lines</li>
<li>解块 Deblock</li>
<li>调整大小 Resize</li>
<li>降噪 Denoise</li>
<li>抗锯齿 Anti-aliasing</li>
<li>去晕 Dering (dehalo)</li>
<li>解带 Deband</li>
<li>颗粒 Grain</li>
<li>抖动到输出位深度 Dither to output bit depth</li>
</ol>
<p>请记住，这只是一个一般的建议。
在某些情况下，你可能想偏离这个建议，例如，如果你使用的是KNLMeansCL这样的快速去噪器，你可以在调整大小之前做这个。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><p>对于实拍内容来说，剪裁几乎总是必要的，因为黑条几乎总是应用于16:9的框架。</p>
<p>如果要裁剪的内容在每个方向上的分辨率都是2的倍数，这就是一个非常直接的过程。</p>
<pre><code class="language-py">crop = src.std.Crop(top=2, bottom=2, left=2, right=2)
</code></pre>
<p>然而，当情况不是这样时，事情就变得有点复杂了。
无论怎样，你都必须在继续操作前尽可能多地裁剪掉。</p>
<p>如果你使用的是完全黑白的影片，请阅读<a href="filtering/../appendix/gray.html">关于这些的附录条目</a>。
下面的内容只适用于有颜色的视频。</p>
<p>然后，阅读本指南中脏线分章的<a href="filtering/dirty_lines.html#fillborders"><code>FillBorders</code></a>解释。
这将解释如果你不打算调整大小，该怎么做。</p>
<p>如果你打算调整大小。
根据 dirty lines 分章中的 <a href="filtering/dirty_lines.html#resizing">调整大小说明</a> 进行操作。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><p>调整大小(Resizing)是一个非常复杂的话题。
然而，对于简单的downscaled压制，人们不需要知道太多信息。
因此，此页面将仅涵盖必要的downscaling说明。
那些有兴趣了解有关重采样的更多信息的人应该参考 <a href="https://guide.encode.moe/encoding/resampling.html">Irrational Encoding Wizardry's guide's resampling page</a> 以获取更多信息。</p>
<p>并且您可以查看后面的子章节以了解一些更高级的主题， 例如 <a href="filtering/descaling.html">解放缩(descaling)与重新缩放(rescaling)</a> 或 <a href="filtering/chroma_res.html">色度重新采样(chroma resampling)和移动(shifting)</a>，这两者都是编码动画时绝对需要了解的。</p>
<h1 id="downscaling"><a class="header" href="#downscaling">Downscaling</a></h1>
<p>对于downscaling, 首选的调整器是 spline36 调整器:</p>
<pre><code class="language-py">resize = src.resize.Spline36(1280, 720, dither_type=&quot;error_diffusion&quot;)
</code></pre>
<p>这里的参数应该很简单：只需根据需要调整宽度和高度。
不要担心 <code>dither_type=&quot;error_diffusion&quot;</code>，只需保持原样即可；它所做的只是让输出看起来更漂亮。
该参数的解释可以在 <a href="filtering/bit_depths.html">抖动(dithering)</a> 章节中找到。</p>
<h2 id="finding-target-dimensions"><a class="header" href="#finding-target-dimensions">Finding target dimensions</a></h2>
<p>对于 16:9 内容，标准分辨率所需的尺寸应该是众所周知的： \(3840\times2160\) 是 2160p, \(1920\times1080\) 是 1080p, \(1280\times720\) 是 720p.</p>
<p>但是，大多数电影不是以这种纵横比制作的。
更常见的纵横比是 2.39:1，其中视频是 \(2048\times858\).
消费产品通常采用上述分辨率，因此在裁剪黑条后更有可能看到类似 \(1920\times804\) 。</p>
<p>由此我们可以推算720p下的尺寸是 \(1280\times536\):</p>
<p>\[\begin{align}
w &amp;= \frac{720}{1080}\times1920=1280 \\
h &amp;= \frac{720}{1080}\times804 =536
\end{align}
\]</p>
<p>然而，情况并非总是如此。
假设您的视频源是 \(1920\times806\):</p>
<p>\[\begin{align}
w &amp;= \frac{720}{1080}\times1920=1280 \\
h &amp;= \frac{720}{1080}\times806 =537.\overline{3}
\end{align}
\]</p>
<p>显然，我们不能调整大小至 \(537.\overline{3}\)，因此我们需要找到具有最低纵横比误差的最接近的高度。
这里的解决方法是除以二，取整，然后再乘以二：</p>
<p>\[
h = \mathrm{round}\left( \frac{720}{1080} \times 806 \times \frac{1}{2} \right) \times 2 = 538
\]</p>
<p>在 Python 中：</p>
<pre><code class="language-py">height = round(1280 / src.width / 2 * src.height) * 2
</code></pre>
<p>现在，我们将其提供给我们的resize：</p>
<pre><code class="language-py">resize = src.resize.Spline36(1280, height, dither_type=&quot;error_diffusion&quot;)
</code></pre>
<p>或者，如果我们的源被裁剪在左侧和右侧而不是顶部和底部，我们会这样做：</p>
<pre><code class="language-py">width = round(720 / src.height / 2 * src.width) * 2
</code></pre>
<p>如果您不想为此烦恼， 您可以使用 <a href="https://git.concertos.live/AHD/awsmfunc/"><code>awsmfunc</code></a> 里封装好的 <code>zresize</code> 方法:</p>
<pre><code class="language-py">resize = awf.zresize(src, preset=720)
</code></pre>
<p>有了这个 <code>preset</code> 选项，您不必费心计算任何事情，只需说明目标分辨率（高度），它就会为您确定正确的尺寸。</p>
<h2 id="笔记"><a class="header" href="#笔记">笔记</a></h2>
<p>如果调整不均匀源的大小， 请参阅 <a href="filtering/dirty_lines.html">脏线(dirty lines)</a> 章节， 特别是 <a href="filtering/dirty_lines.html#fillborders">填充边框(FillBorders)</a> 部分和 <a href="filtering/dirty_lines.html#notes">注解</a>.</p>
<p>此外，值得注意的是，不应在脚本开始时调整大小，因为这样做会损坏执行的某些filtering，甚至重新引入问题。</p>
<h1 id="理想的分辨率"><a class="header" href="#理想的分辨率">理想的分辨率</a></h1>
<p>对于动漫作品，请参阅 <a href="filtering/descaling.html">descaling subchapter</a> 。
实拍场景中极少需要进行descaling，但如果您的来源特别模糊且明显是廉价制作，则值得研究。</p>
<p>众所周知，并非每个源都应以源的分辨率进行压制。
因此，人们应该知道来源是否有保证，例如从细节保留的角度来看，1080p 压制或 720p 压制就足够了。</p>
<p>为此，我们只需要先将源缩小再放大后进行简单比较：</p>
<pre><code class="language-py">downscale = src.resize.Spline36(1280, 720, dither_type=&quot;error_diffusion&quot;)
rescale = downscale.resize.Spline36(src.width, src.height, dither_type=&quot;error_diffusion&quot;)
</code></pre>
<p>现在，我们将两者交错，然后通过视频查看细节是否模糊：</p>
<pre><code class="language-py">out = core.std.Interleave([src, rescale])
</code></pre>
<p>我们还可以使用 <code>awsmfunc</code> 里封装的所有关于 <code>UpscaleCheck</code> 的方法:</p>
<pre><code class="language-py">out = awf.UpscaleCheck(src)
</code></pre>
<p>让我们看两个例子。
第一个，Shinjuku Swan II:</p>
<p align="center">
<img src='Pictures/swan_0.png' onmouseover="this.src='Pictures/swan_1.png';" onmouseout="this.src='Pictures/swan_0.png';"/>
</p>
<p>在这里，边缘在重新调整后变得非常模糊，这意味着 1080p 是有保证的。
这在植物的叶子中尤为明显。</p>
<p>第二个，The Way of the Dragon:</p>
<p align="center">
<img src='Pictures/dragon_0.png' onmouseover="this.src='Pictures/dragon_1.png';" onmouseout="this.src='Pictures/dragon_0.png';"/>
</p>
<p>在这里，我们看到颗粒非常轻微地模糊，并且一些压缩产物被扭曲了。
但是，边缘和细节不受影响，这意味着 720p 在这里就可以了。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="解放缩-descaling"><a class="header" href="#解放缩-descaling">解放缩 Descaling</a></h1>
<p>如果你读过一些关于动漫编码的文章，你可能听说过 &quot;解放缩&quot;这个词；这是一个通过找到原始分辨率和使用的调整内核来 &quot;逆转 &quot;升频的过程。
如果操作正确，这是一个近乎无损的过程，并产生比标准 Spline36 调整大小更清晰的输出，并减少光晕伪影。
然而，如果做得不对，这只会增加已经存在的问题，如光晕、振铃等。</p>
<p>最常用的反比例插件是<a href="https://github.com/Irrational-Encoding-Wizardry/vapoursynth-descale">Descale</a>，它最容易通过<a href="https://github.com/Irrational-Encoding-Wizardry/fvsfunc"><code>fvsfunc</code></a>调用，它对每个内核都有一个别名，例如<code>fvf.Debilinear</code>。
这支持双曲线、双线性、兰佐斯和样条曲线的升尺度。</p>
<p>大多数数字制作的动漫内容，特别是电视节目，都是由720p、810p、864p、900p或介于两者之间的双线性或双立方体升格而成。
虽然不是只有动漫可以做，但它在此类内容中更为普遍，所以我们将相应地关注动漫。</p>
<p>作为我们的例子，我们将看看Nichijou，它是一个从720p的双线升级。</p>
<p>为了展示解放缩的效果，让我们把它与标准的样条调整尺寸进行比较:</p>
<pre><code class="language-py">descale = fvf.Debilinear(src, 1280, 720)
spline = src.resize.Spline36(1280, 720)
out = core.std.Interleave([descale, spline])
</code></pre>
<p align="center"> 
<img src='Pictures/descale0.png' onmouseover="this.src='Pictures/descale1.png';" onmouseout="this.src='Pictures/descale0.png';" />
</p>
<h2 id="原生分辨率和内核"><a class="header" href="#原生分辨率和内核">原生分辨率和内核</a></h2>
<p>现在，当你想降级时，你需要做的第一件事是弄清楚用什么来调整视频的大小，以及从哪个分辨率来调整大小。
这方面最流行的工具是<a href="https://github.com/Infiziert90/getnative">getnative</a>，它允许你给它提供一张图片，然后它将对其进行降级、调整大小，并计算出与来源的差异，然后绘制出结果，这样你就可以找到原始分辨率。</p>
<p>为了使其发挥最大的作用，你要找到一个明亮的画面，并有非常少的模糊、视觉效果、纹理等。</p>
<p>一旦你找到了一个，你就可以按以下方式运行脚本:</p>
<pre><code class="language-sh">python getnative.py image.png -k bilinear
</code></pre>
<p>这将在<code>Results</code>目录下输出一个图形，并猜测其分辨率。
不过，自己看一下这个图是很有必要的。
在我们的例子中，这些是正确的参数，所以我们得到以下结果。</p>
<p><img src="filtering/Pictures/descalebilinear.svg" alt="alt text" title="getnative bilinear graph" /></p>
<p>在720p时有一个明显的下降。
我们还可以测试其他内核:</p>
<pre><code class="language-sh">python getnative.py image.png -k bicubic -b 0 -c 1
</code></pre>
<p>然后，该图看起来如下:</p>
<p><img src="filtering/Pictures/descalesharpbicubic.svg" alt="alt text" title="getnative sharp bicubic graph" /></p>
<p>如果你想测试所有可能的内核，你可以使用<code>--mode &quot;all&quot;</code>。</p>
<p>为了仔细检查，我们将输入的帧与用相同内核放大的解放缩进行比较:</p>
<pre><code class="language-py">descale = fvf.Debilinear(src, 1280, 720)
rescale = descale.resize.Bilinear(src, src.width, src.height)
merge_chroma = rescale.std.Merge(src, [0, 1])
out = core.std.Interleave([src, merge_chroma])
</code></pre>
<p>在这里，我们将源头的色度与我们的重新缩放合并起来，因为色度的分辨率比源头的分辨率低，所以我们不能降低它的比例。
结果:</p>
<p align="center"> 
<img src='Pictures/resize0.png' onmouseover="this.src='Pictures/resize1.png';" onmouseout="this.src='Pictures/resize0.png';" />
</p>
<p>正如你所看到的，线性图实际上是相同的，没有引入额外的光晕或混叠。</p>
<p>另一方面，如果我们尝试一个不正确的内核和分辨率，我们会在重新缩放的图像中看到更多的伪影:</p>
<pre><code class="language-py">b, c = 0, 1
descale = fvf.Debicubic(src, 1440, 810, b=b, c=c)
rescale = descale.resize.Bicubic(src, src.width, src.height, filter_param_a=b, filter_param_b=c)
merge_chroma = rescale.std.Merge(src, [0, 1])
out = core.std.Interleave([src, merge_chroma])
</code></pre>
<p align="center"> 
<img src='Pictures/resize0.png' onmouseover="this.src='Pictures/resize2.png';" onmouseout="this.src='Pictures/resize0.png';" />
</p>
<h1 id="混合分辨率"><a class="header" href="#混合分辨率">混合分辨率</a></h1>
<p>上面关于不正确的内核和高度的例子应该很明显，不正确的解放缩是相当有破坏性的。
不幸的是，大多数可以被降级的视频都有其他分辨率的元素。
有时，一帧中的不同元素会有不同的分辨率，例如，背景是900p的，人物A是810p的，人物B是720p的。
在这样的情况下，通常做一个简单的 spline36 调整大小是比较安全的。
从技术上讲，人们可以做大量的遮罩来解决这个问题，但这是一个很大的努力，而且遮罩很可能会失败。</p>
<p>一个更常见的会遇到混合分辨率的情况是片头和叠加，这通常是1080p的。
让我们来看看，如果我们在上面的画面中添加一些文字，并将其与 spline36 调整进行对比，会发生什么。
为了便于比较，这些图片被放大了3倍:</p>
<p align="center"> 
<img src='Pictures/descalesub0.png' onmouseover="this.src='Pictures/descalesub1.png';" onmouseout="this.src='Pictures/descalesub0.png';" />
</p>
<p>去线性调整在这里明显增加了更强的光晕伪影。</p>
<p>为了处理这个问题，我们可以使用<code>fvsfunc</code>中的<code>DescaleM</code>函数，它掩盖了这些元素，并通过 spline36 调整它们的大小。</p>
<pre><code class="language-py">descale = fvf.DebilinearM(src, 1280, 720)
</code></pre>
<p>由于这些函数相对较慢，你可能要考虑事先找到这些元素，并只对这些帧应用该函数。
如果你不确定你的帧没有1080p元素，但还是坚持使用这些函数。</p>
<p>另外，在非常罕见的情况下，分辨率和/或内核会逐个场景变化，甚至更糟，逐个帧变化。
你可以考虑试试<code>lvsfunc.scale.descale</code>，它试图为每一帧找到理想的高度。
然而，理想的情况是，你应该手动完成这个工作。</p>
<h2 id="444-and-420"><a class="header" href="#444-and-420">4:4:4 and 4:2:0</a></h2>
<h1 id="upscaling-and-rescaling"><a class="header" href="#upscaling-and-rescaling">Upscaling and Rescaling</a></h1>
<h2 id="upscaling"><a class="header" href="#upscaling">Upscaling</a></h2>
<h2 id="rescaling"><a class="header" href="#rescaling">Rescaling</a></h2>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="色度重采样与偏移chroma-resampling-and-shifting"><a class="header" href="#色度重采样与偏移chroma-resampling-and-shifting">色度重采样与偏移(Chroma Resampling and Shifting)</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="位深-bit-depths-简介"><a class="header" href="#位深-bit-depths-简介">位深 Bit Depths: 简介</a></h1>
<p>当你过滤一个画面时，结果被限制在你的比特深度中的可用值。默认情况下，大多数SDR内容为8位，HDR内容为10位。在8位中，你被限制在0和255之间的值。然而，由于大多数视频内容是在有限的范围内，这个范围成为16至235(亮度)和16至240(色度)。</p>
<p>比方说，你想把数值在60到65之间的每个像素提高到0.88的幂。
四舍五入到小数点后3位。</p>
<table><thead><tr><th align="center">Original</th><th align="center">Raised</th></tr></thead><tbody>
<tr><td align="center">60</td><td align="center">36.709</td></tr>
<tr><td align="center">61</td><td align="center">37.247</td></tr>
<tr><td align="center">62</td><td align="center">37.784</td></tr>
<tr><td align="center">63</td><td align="center">38.319</td></tr>
<tr><td align="center">64</td><td align="center">38.854</td></tr>
<tr><td align="center">65</td><td align="center">39.388</td></tr>
</tbody></table>
<p>由于我们仅限于 0 到 255 之间的整数值，因此将这些四舍五入为 37、37、38、38、39、39。
因此，虽然过滤器不会导致相同的值，但我们将这些四舍五入为相同的值。
这会导致生成一些不需要的 <a href="filtering/debanding.html">色带(banding)</a> 。
例如，提高到 8 位的 0.88 次方与 32 位的更高位深度：</p>
<p align="center"> 
<img src='Pictures/gamma_lbd.png' onmouseover="this.src='Pictures/gamma_hbd.png';" onmouseout="this.src='Pictures/gamma_lbd.png';" />
</p>
<p>为了缓解这种情况，我们在更高的位深度下工作，然后使用所谓的抖动算法在舍入期间添加一些波动并防止产生色带。
通常的位深度是 16 位和 32 位。
虽然 16 位一开始听起来更糟，但差异并不明显，并且 32 位，浮点数而不是整数格式，并不是每个过滤器都支持。</p>
<p>幸运的是，对于那些不在更高位深度下工作的人来说，许多过滤器在内部强制更高的精度并正确地抖动结果。
但是，多次在位深度之间切换会浪费 CPU 周期，并且在极端情况下还会改变图像。</p>
<h2 id="更改位深-changing-bit-depths"><a class="header" href="#更改位深-changing-bit-depths">更改位深 Changing bit depths</a></h2>
<p>要在更高的位深度下工作，您可以在脚本filter部分的开头和结尾使用<code>vsutil</code>库中的 <code>depth</code> 方法。
默认情况下，这将使用高质量的抖动算法，并且只需要几次击键：</p>
<pre><code class="language-py">from vsutil import depth

src = depth(src, 16)

resize = ...

my_great_filter = ...

out = depth(my_great_filter, 8)
</code></pre>
<p>当您在更高位深度下工作时，重要的是要记住，某些函数可能需要 8 位的参数输入值，而其他函数则需要输入位深度。
如果您在期望 16 位输入的函数中错误地输入假设为 8 位的 255，您的结果将大不相同，因为 255 是 8 位中较高的值，而在 16 位中，这大致相当于 8位 中的 1。</p>
<p>要转换值，你可以使用<code>vsutil</code>库中的 <code>scale_value</code>方法, 这将有助于处理边缘情况等：</p>
<pre><code class="language-py">from vsutil import scale_value

v_8bit = 128

v_16bit = scale_value(128, 8, 16)
</code></pre>
<p>得到 <code>v_16bit = 32768</code>, 16 位的中间点。</p>
<p>这对于 32 位浮点数并不那么简单，因为您需要指定是否根据范围缩放偏移量以及缩放亮度还是色度。
这是因为有限范围的亮度值介于 0 和 1 之间，而色度值介于 -0.5 和 +0.5 之间。
通常，您将处理电视范围，因此设置 <code>scale_offsets=True</code>:</p>
<pre><code class="language-py">from vsutil import scale_value

v_8bit = 128

v_32bit_luma = scale_value(128, 8, 32, scale_offsets=True)
v_32bit_chroma = scale_value(128, 8, 32, scale_offsets=True, chroma=True)
</code></pre>
<p>得到 <code>v_32bit_luma = 0.5, v_32bit_chroma = 0</code>.</p>
<h1 id="dither-algorithms"><a class="header" href="#dither-algorithms">Dither Algorithms</a></h1>
<p>TODO</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="解带-debanding"><a class="header" href="#解带-debanding">解带 Debanding</a></h1>
<p>这是人们会遇到的最常见的问题。当码率不足(bitstarving)和糟糕的设置导致平滑的渐变变成突然的颜色变化时，通常会产生色带，这显然会让画面看起来很糟糕。这些可以通过执行类似模糊的操作并限制它们的输出来修复。</p>
<p>请注意，由于模糊是一个非常具有破坏性的过程，因此建议仅将其应用于视频的必要部分并使用 <a href="filtering/masking.html">蒙版(masks)</a>来进一步限制更改。</p>
<p>VapourSynth 有三个很棒的工具可以用来修复色带：<a href="https://github.com/HomeOfAviSynthPlusEvolution/neo_f3kdb/"><code>neo_f3kdb</code></a>, <code>fvsfunc</code>内置的蒙版 <code>gradfun3</code> 和 <code>vs-placebo</code>的 <code>placebo.Deband</code>。</p>
<p align="center">
<img src='Pictures/debanding0.png' onmouseover="this.src='Pictures/debanding1.png';" onmouseout="this.src='Pictures/debanding0.png';" />
</p>
<p align="center">
<i>使用 f3kdb 默认设置修复了色带示例</i>
</p>
<h2 id="neo_f3kdb"><a class="header" href="#neo_f3kdb"><code>neo_f3kdb</code></a></h2>
<pre><code class="language-py">deband = core.neo_f3kdb.deband(src=clip, range=15, y=64, cb=64, cr=64, grainy=64, grainc=64, dynamic_grain=False, sample_mode=2)
</code></pre>
<p>这些设置对某些人来说可能不言自明，但它们的作用如下：</p>
<ul>
<li>
<p><code>src</code> 这显然是您的剪辑源。</p>
</li>
<li>
<p><code>range</code> 这指定了用于计算某物是否有条带的像素范围。更大的范围意味着更多的像素用于计算，这意味着它需要更多的处理能力。默认值 15 通常应该没问题。提高此值可能有助于使步长较小的较大梯度看起来更平滑，而较低的值将有助于捕获较小的实例。</p>
</li>
<li>
<p><code>y</code> 最重要的设置，因为大多数（明显的）条带发生在亮度平面上。它指定了亮度平面上的某些东西被认为是色带的差异必须有多大。你应该从低而缓慢的开始，但一定要建立这个直到条带消失。如果设置得太高，很多细节会被视为条带，因此会变得模糊。
根据您的采样模式，值将仅以 16（mode 2）或 32（mode 1、3、4）的步长产生影响。这意味着 y=20 等价于 y=30。</p>
</li>
<li>
<p><code>cb</code> 和 <code>cr</code>除了色度外与 <code>y</code> 都是一样的。 但是，色度平面上的色带相对不常见，因此您通常可以将其关闭。</p>
</li>
<li>
<p><code>grainy</code> 和 <code>grainc</code> 为了防止色带再次发生并抵消平滑，通常在解带后添加颗粒。但是，由于这种假颗粒非常明显，因此建议保守一些。 或者，您可以使用自定义颗粒生成器，这将为您提供更好的输出 (有关更多信息，请参阅 <a href="filtering/graining.html">粒化部分</a>)。</p>
</li>
<li>
<p><code>dynamic_grain</code> 默认情况下，由<code>f3kdb</code>添加的颗粒是静态的，这压缩得更好，因为显然变化较少，但它通常看起来与实况内容无关，因此通常建议将其设置为 <code>True</code> ，除非您正在处理动画内容。</p>
</li>
<li>
<p><code>sample_mode</code> 在README中有说明。因为它可能具有较少的细节损失，可以考虑切换到 4。</p>
</li>
</ul>
<details>
<summary>深入讲解</summary>
TODO
</details>
<h2 id="gradfun3"><a class="header" href="#gradfun3"><code>GradFun3</code></a></h2>
<p><code>gradfun3</code>是 <code>f3kdb</code> 的最受欢迎替代品。 这个函数需要更多的资源和不那么直接的参数，但在一些 <code>f3kdb</code> 处理不好的地方表现不错:</p>
<pre><code class="language-py">import fvsfunc as fvf
deband = fvf.GradFun3(src, thr=0.35, radius=12, elast=3.0, mask=2, mode=3, ampo=1, ampn=0, pat=32, dyn=False, staticnoise=False, smode=2, thr_det=2 + round(max(thr - 0.35, 0) / 0.3), debug=False, thrc=thr, radiusc=radius, elastc=elast, planes=list(range(src.format.num_planes)), ref=src, bits=src.format.bits_per_sample) # + resizing variables
</code></pre>
<p><code>fmtconv</code>中许多设置的值都是给位深转换或解放缩使用的, 这两者在这里都不相关。这里真正感兴趣的值是：</p>
<ul>
<li>
<p><code>thr</code> 等价于 <code>y</code>, <code>cb</code>, 和 <code>cr</code> 的作用。您可能想要提高或降低它。</p>
</li>
<li>
<p><code>radius</code> 具有和<code>f3kdb</code>的 <code>range</code> 相同的效果。</p>
</li>
<li>
<p><code>smode</code> 设置平滑模式。通常最好保留默认值，如果您想使用支持 CUDA 的 GPU 而不是 CPU，则设置为 5。使用 <code>ref</code> (默认为剪辑输入) 作为参考剪辑。</p>
</li>
<li>
<p><code>mask</code> 设置遮罩强度。 0 禁用。 默认值是一个合理的值。</p>
</li>
<li>
<p><code>planes</code> 置应处理哪些平面。</p>
</li>
<li>
<p><code>debug</code> 允许您查看遮罩。</p>
</li>
<li>
<p><code>elast</code> 控制去色带和剪辑源之间的混合。默认值是一个合理的值。
较高的值优先考虑去色带。</p>
</li>
</ul>
<details>
<summary>深入讲解</summary>
TODO
要更深入地解释 `thr` 和 `elast` 的作用, 请查看 <a href=https://github.com/HomeOfVapourSynthEvolution/mvsfunc/blob/master/mvsfunc.py#L1735><code>mvsfunc</code></a>的算法解释.
</details>
<h2 id="placebodeband"><a class="header" href="#placebodeband"><code>placebo.Deband</code></a></h2>
<p>这个 debander 对 VapourSynth 来说很新，但它非常擅长修复强条带。然而，同样地，它也容易出现不必要的细节损失，因此应该只在必要时使用，并且最好与细节/边缘蒙版结合使用。它的（当前）参数：</p>
<pre><code class="language-py">placebo.Deband(clip clip[, int planes = 1, int iterations = 1, float threshold = 4.0, float radius = 16.0, float grain = 6.0, int dither = True, int dither_algo = 0])
</code></pre>
<p>这个功能在未来不太可能发生重大变化，因此非常值得读一读 <a href="https://github.com/Lypheo/vs-placebo/blob/master/README.md">the README</a> 。</p>
<p>您要查看的参数：</p>
<ul>
<li>
<p><code>planes</code> 显然是要加工的平面。此处的语法不同，请查看README。简而言之，默认仅对亮度， <code>1 | 2 | 4</code> 对亮度和色度。</p>
</li>
<li>
<p><code>iterations</code> 设置 debander 循环的频率。 不建议更改默认设置，尽管这在极端情况下很有用。</p>
</li>
<li>
<p><code>threshold</code> 设置 debander 的强度或更改像素时的阈值。尽量不要超过 12。如果会，请以 1 为步长进行微调。</p>
</li>
<li>
<p><code>radius</code> 与之前的功能相同。</p>
</li>
<li>
<p><code>grain</code> 同样与 <code>f3kdb</code> 的一样, 但是颗粒更好。</p>
</li>
</ul>
<details>
<summary>深入讲解</summary>
TODO
它使用了 mpv debander，只是平均一个范围内的像素，如果差异低于阈值，则输出平均值。该算法在 <a href="https://github.com/haasn/libplacebo/blob/master/src/shaders/sampling.c#L167">中进行了解释</a>.
</details>
<h2 id="色带检测-banding-detection"><a class="header" href="#色带检测-banding-detection">色带检测 Banding detection</a></h2>
<p>如果要自动检测色带，可以使用<code>awsmfunc</code>中的 <code>banddtct</code> 。 确保正确调整值并检查完整输出。查看 <a href="https://git.concertos.live/AHD/awsmfunc/wiki/Using-detect.py">此链接</a> 以获取有关如何使用它的说明。您也可以只运行 <code>adptvgrnMod</code> 或用一个高的<code>luma_scaling</code>值来运行 <code>adaptive_grain</code> 以期望颗粒可以完全覆盖它。更多信息在
<a href="filtering/graining">粒化部分</a>。请注意，这两种方法都无法检测/修复所有类型的色带。 <code>banddtct</code> 找不到被颗粒覆盖的色带，而且用于修复色带的纹理仅适用于较小的实例。</p>
<h1 id="解块-deblocking"><a class="header" href="#解块-deblocking">解块 Deblocking</a></h1>
<p align="center">
<img src='Pictures/deblock1.png' onmouseover="this.src='Pictures/deblock2.png';" onmouseout="this.src='Pictures/deblock1.png';"/>
</p>
<p>解块相当于平滑输入源，通常是在画面顶部使用另一个蒙版。 最常用的是<code>havsfunc</code>中的 <code>Deblock_QED</code> 函数。
主要参数是</p>
<ul>
<li>
<p><code>quant1</code>: 边缘解块的强度。 默认值为 24。您可能希望显着提高此值。</p>
</li>
<li>
<p><code>quant2</code>: 内部解块的强度。 默认值为 26。同样，提高此值可能会有益。</p>
</li>
</ul>
<details>
<summary>深入讲解</summary>
TODO
</details>
<p>其他常用的选项是 <code>deblock.Deblock</code>,它非常强大，几乎总是有效</p>
<details>
<summary>深入讲解</summary>
TODO
</details>
<p><code>dfttest.DFTTest</code>, 相对较弱，但非常暴力，还有
<code>fvf.AutoDeblock</code>, 对于 MPEG-2 源的解块非常有用，并且可以应用于整个视频。另一种流行的方法是简单地解带，因为解块和解带非常相似。这对于 AVC 蓝光源是一个不错的选择。</p>
<details>
<summary>深入讲解</summary>
TODO
</details>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="graining"><a class="header" href="#graining">Graining</a></h1>
<p>TODO: explain why we love grain so much and static vs. dynamic grain.  Also, images.</p>
<h1 id="graining-filters"><a class="header" href="#graining-filters">Graining Filters</a></h1>
<p>There are a couple different filters you can use to grain.
As lots of functions work similarly, we will only cover AddGrain and libplacebo graining.</p>
<h2 id="addgrain"><a class="header" href="#addgrain">AddGrain</a></h2>
<p>This plugin allows you to add grain to the luma and chroma grains in differing strengths and grain patterns:</p>
<pre><code class="language-py">grain = src.grain.Add(var=1.0, uvar=0.0, seed=-1, constant=False)
</code></pre>
<p>Here, <code>var</code> controls the grain strength for the luma plane, and <code>uvar</code> controls the strength for the chroma plane.
<code>seed</code> allows you to specify a custom grain pattern, which is useful if you'd like to reproduce a grain pattern multiple times, e.g. for comparing encodes.
<code>constant</code> allows you to choose between static and dynamic grain.</p>
<p>Raising the strength increases both the amount of grain added as well as the offset a grained pixel will have from the original pixel.
For example, <code>var=1</code> will lead to values being up to 3 8-bit steps away from the input values.</p>
<p>There's no real point in using this function directly, but it's good to know what it does, as it's considered the go-to grainer.</p>
<details>
<summary>In-depth function explanation</summary>
This plugin uses a normal distribution to find the values it changes the input by.
The `var` parameter is the standard deviation (usually noted as \(\sigma\)) of the normal distribution.
<p>This means that (these are approximations):</p>
<ul>
<li>\(68.27\%\) of output pixel values are within \(\pm1\times\mathtt{var}\) of the input value</li>
<li>\(95.45\%\) of output pixel values are within \(\pm2\times\mathtt{var}\) of the input value</li>
<li>\(99.73\%\) of output pixel values are within \(\pm3\times\mathtt{var}\) of the input value</li>
<li>\(50\%\) of output pixel values are within \(\pm0.675\times\mathtt{var}\) of the input value</li>
<li>\(90\%\) of output pixel values are within \(\pm1.645\times\mathtt{var}\) of the input value</li>
<li>\(95\%\) of output pixel values are within \(\pm1.960\times\mathtt{var}\) of the input value</li>
<li>\(99\%\) of output pixel values are within \(\pm2.576\times\mathtt{var}\) of the input value</li>
</ul>
</details>
<h2 id="placebodeband-as-a-grainer"><a class="header" href="#placebodeband-as-a-grainer">placebo.Deband as a grainer</a></h2>
<p>Alternatively, using <code>placebo.Deband</code> solely as a grainer can also lead to some nice results:</p>
<pre><code class="language-py">grain = placebo.Deband(iterations=0, grain=6.0)
</code></pre>
<p>The main advantage here is it runs on your GPU, so if your GPU isn't already busy with other filters, using this can get you a slight speed-up.</p>
<details>
<summary>In-depth function explanation</summary>
TODO
</details>
<h2 id="adaptive_grain"><a class="header" href="#adaptive_grain">adaptive_grain</a></h2>
<p>This function from <a href="https://github.com/Irrational-Encoding-Wizardry/kagefunc"><code>kagefunc</code></a> applies AddGrain according to overall frame brightness and individual pixel brightness.
This is very useful for covering up minor banding and/or helping x264 distribute more bits to darks.</p>
<pre><code class="language-py">grain = kgf.adaptive_grain(src, strength=.25, static=True, luma_scaling=12, show_mask=False)
</code></pre>
<p><code>strength</code> here is <code>var</code> from AddGrain.
The default or slightly lower is usually fine.
You likely don't want to go above 0.75.</p>
<p>The <code>luma_scaling</code> parameter is used to control how strong it should favor darker frames over brighter frames, whereby lower <code>luma_scaling</code> will apply more grain to bright frames.
You can use extremely low or extremely high values here depending on what you want.
For example, if you want to grain all frames significantly, you might use <code>luma_scaling=5</code>, while if you just want to apply grain to darker parts of darker frames to cover up minor banding, you might use <code>luma_scaling=100</code>.</p>
<p><code>show_mask</code> shows you the mask that's used to apply the grain, with whiter meaning more grain is applied.
It's recommended to switch this on when tuning <code>luma_scaling</code>.</p>
<details>
<summary>In-depth function explanation</summary>
The author of the function wrote a <a href="https://blog.kageru.moe/legacy/adaptivegrain.html">fantastic blog post explaining the function and how it works</a>.
</details>
<h2 id="grainfactory3"><a class="header" href="#grainfactory3">GrainFactory3</a></h2>
<p>TODO: rewrite this or just remove it.</p>
<p>An older alternative to <code>kgf.adaptive_grain</code>, <a href="https://github.com/HomeOfVapourSynthEvolution/havsfunc"><code>havsfunc</code></a>'s <code>GrainFactory3</code> is still quite interesting.
It splits pixel values into four groups based on their brightness and applies differently sized grain at different strengths via AddGrain to these groups.</p>
<pre><code class="language-py">grain = haf.GrainFactory3(src, g1str=7.0, g2str=5.0, g3str=3.0, g1shrp=60, g2shrp=66, g3shrp=80, g1size=1.5, g2size=1.2, g3size=0.9, temp_avg=0, ontop_grain=0.0, th1=24, th2=56, th3=128, th4=160)
</code></pre>
<p>The parameters are explained <a href="https://github.com/HomeOfVapourSynthEvolution/havsfunc/blob/master/havsfunc.py#L3720">above the source code</a>.</p>
<p>This function is mainly useful if you want to apply grain to specific frames only, as overall frame brightness should be taken into account if grain is applied to the whole video.</p>
<p>For example, <code>GrainFactory3</code> to make up for missing grain on left and right borders:</p>
<p align="center"> 
<img src='Pictures/grain0.png' onmouseover="this.src='Pictures/grain1.png';" onmouseout="this.src='Pictures/grain0.png';" />
</p>
<details>
<summary>In-depth function explanation</summary>
TODO
<p>In short: Create a mask for each brightness group, use bicubic resizing with sharpness controlling b and c to resize the grain, then apply that.
Temporal averaging just averages the grain for the current frame and its direct neighbors using misc.AverageFrames.</p>
</details>
<h2 id="adptvgrnmod"><a class="header" href="#adptvgrnmod">adptvgrnMod</a></h2>
<p>This function resizes grain in the same way <code>GrainFactory3</code> does, then applies it using the method from <code>adaptive_grain</code>.
It also has some protection for darks and brights to maintain average frame brightness:</p>
<pre><code class="language-py">grain = agm.adptvgrnMod(strength=0.25, cstrength=None, size=1, sharp=50, static=False, luma_scaling=12, seed=-1, show_mask=False)
</code></pre>
<p>Grain strength is controlled by <code>strength</code> for luma and <code>cstrength</code> for chroma.
<code>cstrength</code> defaults to half of <code>strength</code>.
Just like <code>adaptive_grain</code>, the default or slightly lower is usually fine, but you shouldn't go too high.
If you're using a <code>size</code> greater than the default, you can get away with higher values, e.g. <code>strength=1</code>, but it's still advised to stay conservative with grain application.</p>
<p>The <code>size</code> and <code>sharp</code> parameters allow you to make the applied grain look a bit more like the rest of the film's.
It's recommended to play around with these so that fake grain isn't too obvious.
In most cases, you will want to raise both of them ever so slightly, e.g. <code>size=1.2, sharp=60</code>.</p>
<p><code>static</code>, <code>luma_scaling</code>, and <code>show_mask</code> are equivalent to <code>adaptive_grain</code>, so scroll up for explanations.
<code>seed</code> is the same as AddGrain's; again, scroll up.</p>
<p>By default, <code>adptvgrnMod</code> will fade grain around extremes (16 or 235) and shades of gray.
These features can be turned off by setting <code>fade_edges=False</code> and <code>protect_neutral=False</code> respectively.</p>
<p>It's recently become common practice to remove graining entirely from one's debander and grain debanded areas entirely with this function.</p>
<h3 id="sizedgrn"><a class="header" href="#sizedgrn">sizedgrn</a></h3>
<p>If one wants to disable the brightness-based application, one can use <code>sizedgrn</code>, which is the internal graining function in <code>adptvgrnMod</code>.</p>
<details>
<summary>Some examples of <code>adptvgrnMod</code> compared with <code>sizedgrn</code> for those curious</summary>
<p>A bright scene, where the brightness-based application makes a large difference:</p>
<p align="center"> 
<img src='Pictures/graining4.png' onmouseover="this.src='Pictures/graining7.png';" onmouseout="this.src='Pictures/graining4.png';" />
</p>
<p>An overall darker scene, where the difference is a lot smaller:</p>
<p align="center"> 
<img src='Pictures/graining3.png' onmouseover="this.src='Pictures/graining6.png';" onmouseout="this.src='Pictures/graining3.png';" />
</p>
<p>A dark scene, where grain is applied evenly (almost) everywhere in the frame:</p>
<p align="center"> 
<img src='Pictures/graining5.png' onmouseover="this.src='Pictures/graining8.png';" onmouseout="this.src='Pictures/graining5.png';" />
</p>
</details>
<details>
<summary>In-depth function explanation</summary>
(Old write-up from the function's author.)
<h3 id="size-and-sharpness"><a class="header" href="#size-and-sharpness">Size and Sharpness</a></h3>
<p>The graining part of adptvgrnMod is the same as GrainFactory3's; it creates a &quot;blank&quot; (midway point of bit depth) clip at a resolution defined by the size parameter, then scales that via a bicubic kernel that uses b and c values determined by sharp:</p>
<p>$$\mathrm{grain\ width} = \mathrm{mod}4 \left( \frac{\mathrm{clip\ width}}{\mathrm{size}} \right)$$</p>
<p>For example, with a 1920x1080 clip and a size value of 1.5:</p>
<p>$$ \mathrm{mod}4 \left( \frac{1920}{1.5} \right) = 1280 $$</p>
<p>This determines the size of the frame the grainer operates on.</p>
<p>Now, the bicubic kernel's parameters are determined:</p>
<p>$$ b = \frac{\mathrm{sharp}}{-50} + 1 $$
$$ c = \frac{1 - b}{2} $$</p>
<p>This means that for the default sharp of 50, a Catmull-Rom filter is used:</p>
<p>$$ b = 0, \qquad c = 0.5 $$</p>
<p>Values under 50 will tend towards B-Spline (b=1, c=0), while ones above 50 will tend towards b=-1, c=1. As such, for a Mitchell (b=1/3, c=1/3) filter, one would require sharp of 100/3.</p>
<p>The grained &quot;blank&quot; clip is then resized to the input clip's resolution with this kernel. If size is greater than 1.5, an additional resizer call is added before the upscale to the input resolution:</p>
<p>$$ \mathrm{pre\ width} = \mathrm{mod}4 \left( \frac{\mathrm{clip\ width} + \mathrm{grain\ width}}{2} \right) $$</p>
<p>With our resolutions so far (assuming we did this for size 1.5), this would be 1600.  This means with size 2, where this preprocessing would actually occur, our grain would go through the following resolutions:</p>
<p>$$ 960 \rightarrow 1440 \rightarrow 1920 $$</p>
<h3 id="fade-edges"><a class="header" href="#fade-edges">Fade Edges</a></h3>
<p>The fade_edges parameter introduces the option to attempt to maintain overall average image brightness, similar to ideal dithering. It does so by limiting the graining at the edges of the clip's range. This is done via the following expression:</p>
<pre><code>x y neutral - abs - low &lt; x y neutral - abs + high &gt; or
x y neutral - x + ?
</code></pre>
<p>Here, x is the input clip, y is the grained clip, neutral is the midway point from the previously grained clip, and low and high are the edges of the range (e.g. 16 and 235 for 8-bit luma). Converted from postfix to infix notation, this reads:</p>
<p>\[x = x\ \mathtt{if}\ x - \mathrm{abs}(y - neutral) &lt; low\ \mathtt{or}\ x - \mathrm{abs}(y - neutral) &gt; high\ \mathtt{else}\ x + (y - neutral)\]</p>
<p>The effect here is that all grain that wouldn't be clipped during output regardless of whether it grains in a positive or negative direction remains, while grain that would pass the plane's limits isn't taken.</p>
<p>In addition to this parameter, protect_neutral is also available. This parameter protects &quot;neutral&quot; chroma (i.e. chroma for shades of gray) from being grained. To do this, it takes advantage of AddGrainC working according to a Guassian distribution, which means that
$$max\ value = 3 \times \sigma$$
(sigma being the standard deviation - the strength or cstrength parameter) is with 99.73% certainty the largest deviated value from the norm (0). This means we can perform a similar operation to the one for fade_edges to keep the midways from being grained. To do this, we resize the input clip to 4:4:4 and use the following expression:</p>
<p>\[\begin{align}x \leq (low + max\ value)\ \mathtt{or}\ x \geq (high - max\ value)\ \mathtt{and}\\ \mathrm{abs}(y - neutral) \leq max\ value\ \mathtt{and}\ \mathrm{abs}(z - neutral) \leq max\ value \end{align}\]</p>
<p>With x, y, z being each of the three planes. If the statement is true, the input clip is returned, else the grained clip is returned.</p>
<p>I originally thought the logic behind protect_neutral would also work well for fade_edges, but I then realized this would completely remove grain near the edges instead of fading it.</p>
<p>Now, the input clip and grained clip (which was merged via std.MergeDiff, which is x - y - neutral) can be merged via the adaptive_grain mask.</p>
</details>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><p align="center">
<img src='Pictures/dirt_source.png' onmouseover="this.src='Pictures/dirt_filtered.png';" onmouseout="this.src='Pictures/dirt_source.png';"/>
</p>
<p align="center">
<i>来自 A Silent Voice (2016)的前奏的脏线。鼠标移上去: 用ContinuityFixer和FillBorders修复。</i>
</p>
<p>你可能会遇到的一个更常见的问题是 &quot;脏线&quot;，这通常是在视频的边界上发现的，其中一排或一列的像素表现出与周围环境不一致的亮度值。通常情况下，这是由于不当的downscaling，例如在添加边框后downscaling。脏线也可能发生，因为压缩者没有考虑到他们在使用4:2:2色度子采样时（意味着他们的高度不必是mod2），消费者的视频将是4:2:0，导致额外的黑行，如果主片段没有正确放置，你就无法在裁剪时摆脱。另一种形式的脏线是在黑条上出现色度平面时表现出来的。通常情况下，这些应该被裁剪掉。然而，相反的情况也可能发生，即具有合法的 luma 信息的平面缺乏色度信息。</p>
<p>重要的是要记住，有时你的来源会有假行（通常被称为 &quot;死&quot;行），也就是没有合法信息的行。这些通常只是镜像下一行/一列。不要麻烦地修复这些，只需裁剪它们。一个例子:</p>
<p align="center">
<img src='Pictures/dead_lines.png'/>
</p>
<p>同样，当你试图修复脏线时，你应该彻底检查你的修复没有引起不必要的问题，如涂抹（常见于过度热心的ContinuityFixer值）或闪烁（特别是在片头，在大多数情况下，建议从你的修复中省略片头卷）。如果你不能找出适当的修复方法，完全可以裁剪掉脏线或不修复。糟糕的修复比没有修复更糟糕</p>
<p>这里有五种常用的修复脏线的方法:</p>
<h2 id="rektlvls"><a class="header" href="#rektlvls"><code>rektlvls</code></a></h2>
<p>来自<a href="https://gitlab.com/Ututu/rekt"><code>rekt</code></a>。这基本上是AviSynth的 <code>FixBrightnessProtect3</code> 和 <code>FixBrightness</code> 的合二为一，尽管与  <code>FixBrightness</code> 不同，不是对整个画面进行处理。它的数值很直接。提高调整值是为了变亮，降低是为了变暗。将<code>prot_val</code>设置为<code>None</code>，它的功能就像<code>FixBrightness</code>，意味着调整值需要改变。</p>
<pre><code class="language-py">from rekt import rektlvls
fix = rektlvls(src, rownum=None, rowval=None, colnum=None, colval=None, prot_val=[16, 235])
</code></pre>
<p>如果你想一次处理多行，你可以输入一个列表 (例如 <code>rownum=[0, 1, 2]</code>).</p>
<p>为了说明这一点，让我们看看《寄生虫》（2017）的黑白蓝光中的脏线。寄生虫（2019）的底层行的黑白蓝光:</p>
<p align="center">
<img src='Pictures/rektlvls_src.png';"/>
</p>
<p>在这个例子中，最下面的四行有交替的亮度 与下两行的偏移量。所以，我们可以用<code>rektlvls</code>来提高 提高第一行和第三行的luma，然后再降低第二行和第四行的luma。在第二和第四行中降低。</p>
<pre><code class="language-py">fix = rektlvls(src, rownum=[803, 802, 801, 800], rowval=[27, -10, 3, -3])
</code></pre>
<p>在这种情况下，我们处于<code>FixBrightnessProtect3</code>模式。我们在这里没有利用<code>prot_val</code>的优势，但人们通常会使用这种模式，因为总有机会帮助我们。结果是:</p>
<p align="center">
<img src='Pictures/rektlvls_fix.png' onmouseover="this.src='Pictures/rektlvls_src.png';" onmouseout="this.src='Pictures/rektlvls_fix.png';"/>
</p>
<details>
<summary>深入功能讲解</summary>
In <code>FixBrightness</code> mode, this will perform an adjustment with
<a href="filtering/www.vapoursynth.com/doc/functions/levels.html"><code>std.Levels</code></a> on the desired row. This means that, in 8-bit,
every possible value \(v\) is mapped to a new value according to the
following function: 
$$\begin{aligned}
&\forall v \leq 255, v\in\mathbb{N}: \\
&\max\left[\min\left(\frac{\max(\min(v, \texttt{max_in}) - \texttt{min_in}, 0)}{(\texttt{max_in} - \texttt{min_in})}\times (\texttt{max_out} - \texttt{min_out}) + \texttt{min_out}, 255\right), 0\right] + 0.5
\end{aligned}$$
For positive <code>adj_val</code>,
\(\texttt{max_in}=235 - \texttt{adj_val}\). For negative ones,
\(\texttt{max_out}=235 + \texttt{adj_val}\). The rest of the values
stay at 16 or 235 depending on whether they are maximums or
minimums.
<p><code>FixBrightnessProtect3</code> mode takes this a bit further, performing
(almost) the same adjustment for values between the first
\(\texttt{prot_val} + 10\) and the second \(\texttt{prot_val} - 10\),
where it scales linearly. Its adjustment value does not work the
same, as it adjusts by \(\texttt{adj_val} \times 2.19\).  In 8-bit:</p>
<p>Line brightening:
$$\begin{aligned}
&amp;\texttt{if }v - 16 &lt;= 0 \\
&amp;\qquad 16 / \\
&amp;\qquad \texttt{if } 235 - \texttt{adj_val} \times 2.19 - 16 &lt;= 0 \\
&amp;\qquad \qquad 0.01 \\
&amp;\qquad \texttt{else} \\
&amp;\qquad \qquad 235 - \texttt{adj_val} \times 2.19 - 16 \\
&amp;\qquad \times 219 \\
&amp;\texttt{else} \\
&amp;\qquad (v - 16) / \\
&amp;\qquad \texttt{if }235 - \texttt{adj_val} \times 2.19 - 16 &lt;= 0 \\
&amp;\qquad \qquad 0.01 \\
&amp;\qquad \texttt{else} \\
&amp;\qquad \qquad 235 - \texttt{adj_val} \times 2.19 - 16 \\
&amp;\qquad \times 219 + 16
\end{aligned}$$</p>
<p>Line darkening:
$$\begin{aligned}
&amp;\texttt{if }v - 16 &lt;= 0 \\
&amp;\qquad\frac{16}{219} \times (235 + \texttt{adj_val} \times 2.19 - 16) \\
&amp;\texttt{else} \\
&amp;\qquad\frac{v - 16}{219} \times (235 + \texttt{adj_val} \times 2.19 - 16) + 16 \\
\end{aligned}$$</p>
<p>All of this, which we give the variable \(a\), is then protected by (for simplicity's sake, only doing dual <code>prot_val</code>, noted by \(p_1\) and \(p_2\)):
$$\begin{aligned}
&amp; a \times \min \left[ \max \left( \frac{v - p_1}{10}, 0 \right), 1 \right] \\
&amp; + v \times \min \left[ \max \left( \frac{v - (p_1 - 10)}{10}, 0 \right), 1 \right] \times \min \left[ \max \left( \frac{p_0 - v}{-10}, 0\right), 1 \right] \\
&amp; + v \times \max \left[ \min \left( \frac{p_0 + 10 - v}{10}, 0\right), 1\right] 
\end{aligned}$$</p>
</details>
<h2 id="bbmod"><a class="header" href="#bbmod"><code>bbmod</code></a></h2>
<p>来自<code>awsmfunc</code>。 这是原BalanceBorders函数的一个模子。虽然它不能像<code>rektlvls'那样保留原始数据，但在高</code>blur'和<code>thresh'值的情况下，它可以产生很好的结果，而且很容易用于多行，特别是具有不同亮度的行，</code>rektlvls'就不再有用。如果它不能产生像样的结果，可以改变这些值，但是你设置得越低，这个函数的破坏性就越大。它也比<code>havsfunc</code>和<code>sgvsfunc</code>中的版本快得多，因为只有必要的像素被处理。</p>
<pre><code class="language-py">import awsmfunc as awf
bb = awf.bbmod(src=clip, left=0, right=0, top=0, bottom=0, thresh=[128, 128, 128], blur=[20, 20, 20], planes=[0, 1, 2], scale_thresh=False, cpass2=False)
</code></pre>
<p>The arrays for <code>thresh</code> and <code>blur</code> are again y, u, and v values.
It's recommended to try <code>blur=999</code> first, then lowering that and
<code>thresh</code> until you get decent values.<br />
<code>thresh</code> specifies how far the result can vary from the input. This
means that the lower this is, the better. <code>blur</code> is the strength of
the filter, with lower values being stronger, and larger values
being less aggressive. If you set <code>blur=1</code>, you're basically copying
rows. If you're having trouble with chroma, you can try activating
<code>cpass2</code>, but note that this requires a very low <code>thresh</code> to be set,
as this changes the chroma processing significantly, making it quite
aggressive.</p>
<p>For our example, I've created fake dirty lines, which we will fix:</p>
<p align="center">
<img src='Pictures/dirtfixes0.png';"/>
</p>
<p>To fix this, we can apply <code>bbmod</code> with a low blur and a high thresh,
meaning pixel values can change significantly:</p>
<pre><code class="language-py">fix = awf.bbmod(src, top=6, thresh=90, blur=20)
</code></pre>
<p align="center">
<img src='Pictures/dirtfixes1.png' onmouseover="this.src='Pictures/dirtfixes0.png';" onmouseout="this.src='Pictures/dirtfixes1.png';"/>
</p>
<p>Our output is already a lot closer to what we assume the source
should look like. Unlike <code>rektlvls</code>, this function is quite quick to
use, so lazy people (i.e. everyone) can use this to fix dirty lines
before resizing, as the difference won't be noticeable after
resizing.</p>
<p>While you can use <code>rektlvls</code> on as many rows/columns as necessary, the same doesn't hold true for <code>bbmod</code>.  Unless you are resizing after, you should only use <code>bbmod</code> on two rows/pixels for low <code>blur</code> values (\(\approx 20\)) or three for higher <code>blur</code> values.  If you are resizing after, you can change the maximum value according to:
\[
max_\mathrm{resize} = max \times \frac{resolution_\mathrm{source}}{resolution_\mathrm{resized}}
\]</p>
<details>
<summary>In-depth function explanation</summary>
<code>bbmod</code> works by blurring the desired rows, input rows, and
reference rows within the image using a blurred bicubic kernel,
whereby the blur amount determines the resolution scaled to accord
to \(\mathtt{\frac{width}{blur}}\). The output is compared using
expressions and finally merged according to the threshold specified.
<p>The function re-runs one function for the top border for each side by flipping and transposing.  As such, this explanation will only cover fixing the top.</p>
<p>First, we double the resolution without any blurring (\(w\) and \(h\) are input clip's width and height):
\[
clip_2 = \texttt{resize.Point}(clip, w\times 2, h\times 2)
\]</p>
<p align="center">
<img src='Pictures/bbmod0_0.png' />
</p>
<p>Now, the reference is created by cropping off double the to-be-fixed number of rows.  We set the height to 2 and then match the size to the double res clip:
\[\begin{align}
clip &amp;= \texttt{CropAbs}(clip_2, \texttt{width}=w \times 2, \texttt{height}=2, \texttt{left}=0, \texttt{top}=top \times 2) \\
clip &amp;= \texttt{resize.Point}(clip, w \times 2, h \times 2)
\end{align}\]</p>
<p align="center">
<img src='Pictures/bbmod0_1.png' />
</p>
<p>Before the next step, we determine the \(blurwidth\):
\[
blurwidth = \max \left( 8, \texttt{floor}\left(\frac{w}{blur}\right)\right)
\]
In our example, we get 8.</p>
<p>Now, we use a blurred bicubic resize to go down to \(blurwidth \times 2\) and back up:
\[\begin{align}
referenceBlur &amp;= \texttt{resize.Bicubic}(clip, blurwidth \times 2, top \times 2, \texttt{b}=1, \texttt{c}=0) \\
referenceBlur &amp;= \texttt{resize.Bicubic}(referenceBlur, w \times 2, top \times 2, \texttt{b}=1, \texttt{c}=0)
\end{align}\]</p>
<p align="center">
<img src='Pictures/bbmod0_2.png' />
</p>
<p align="center">
<img src='Pictures/bbmod0_3.png' />
</p>
<p>Then, crop the doubled input to have height of \(top \times 2\):
\[
original = \texttt{CropAbs}(clip_2, \texttt{width}=w \times 2, \texttt{height}=top \times 2)
\]</p>
<p align="center">
<img src='Pictures/bbmod0_4.png' />
</p>
<p>Prepare the original clip using the same bicubic resize downwards:
\[
clip = \texttt{resize.Bicubic}(original, blurwidth \times 2, top \times 2, \texttt{b}=1, \texttt{c}=0)
\]</p>
<p align="center">
<img src='Pictures/bbmod0_5.png' />
</p>
<p>Our prepared original clip is now also scaled back down:
\[
originalBlur = \texttt{resize.Bicubic}(clip, w \times 2, top \times 2, \texttt{b}=1, \texttt{c}=0)
\]</p>
<p align="center">
<img src='Pictures/bbmod0_6.png' />
</p>
<p>Now that all our clips have been downscaled and scaled back up, which is the blurring process that approximates what the actual value of the rows should be, we can compare them and choose how much of what we want to use.  First, we perform the following expression (\(x\) is \(original\), \(y\) is \(originalBlur\), and \(z\) is \(referenceBlur\)):
\[
\max \left[ \min \left( \frac{z - 16}{y - 16}, 8 \right), 0.4 \right] \times (x + 16) + 16
\]
The input here is:
\[
balancedLuma = \texttt{Expr}(\texttt{clips}=[original, originalBlur, referenceBlur], \texttt{&quot;z 16 - y 16 - / 8 min 0.4 max x 16 - * 16 +&quot;})
\]</p>
<p align="center">
<img src='Pictures/bbmod0_7.png' />
</p>
<p>What did we do here?  In cases where the original blur is low and supersampled reference's blur is high, we did:
\[
8 \times (original + 16) + 16
\]
This brightens the clip significantly.  Else, if the original clip's blur is high and supersampled reference is low, we darken:
\[
0.4 \times (original + 16) + 16
\]
In normal cases, we combine all our clips:
\[
(original + 16) \times \frac{originalBlur - 16}{referenceBlur - 16} + 16
\]</p>
<p>We add 128 so we can merge according to the difference between this and our input clip:
\[
difference = \texttt{MakeDiff}(balancedLuma, original)
\]</p>
<p>Now, we compare to make sure the difference doesn't exceed \(thresh\):
\[\begin{align}
difference &amp;= \texttt{Expr}(difference, &quot;x thresh &gt; thresh x ?&quot;) \\
difference &amp;= \texttt{Expr}(difference, &quot;x thresh &lt; thresh x ?&quot;)
\end{align}\]</p>
<p>These expressions do the following:
\[\begin{align}
&amp;\texttt{if }difference &gt;/&lt; thresh:\\
&amp;\qquad    thresh\\
&amp;\texttt{else}:\\
&amp;\qquad    difference
\end{align}\]</p>
<p>This is then resized back to the input size and merged using <code>MergeDiff</code> back into the original and the rows are stacked onto the input.  The output resized to the same res as the other images:</p>
<p align="center">
<img src='Pictures/bbmod0_9.png' />
</p>
</details>
<h2 id="fillborders"><a class="header" href="#fillborders"><code>FillBorders</code></a></h2>
<p>From <a href="https://github.com/dubhater/vapoursynth-fillborders"><code>fb</code></a>.  This function pretty much just copies the next column/row in line.
While this sounds, silly, it can be quite useful when downscaling
leads to more rows being at the bottom than at the top, and one
having to fill one up due to YUV420's mod2 height.</p>
<pre><code class="language-py">fill = core.fb.FillBorders(src=clip, left=0, right=0, bottom=0, top=0, mode=&quot;fixborders&quot;)
</code></pre>
<p>A very interesting use for this function is one similar to applying
<code>ContinuityFixer</code> only to chroma planes, which can be used on gray
borders or borders that don't match their surroundings no matter
what luma fix is applied. This can be done with the following
script:</p>
<pre><code class="language-py">fill = core.fb.FillBorders(src=clip, left=0, right=0, bottom=0, top=0, mode=&quot;fixborders&quot;)
merge = core.std.Merge(clipa=clip, clipb=fill, weight=[0,1])
</code></pre>
<p>You can also split the planes and process the chroma planes
individually, although this is only slightly faster. A wrapper that
allows you to specify per-plane values for <code>fb</code> is <code>FillBorders</code> in
<code>awsmfunc</code>.</p>
<p>Note that you should only ever fill single columns/rows with <code>FillBorders</code>.  If you have more black lines, crop them!  If there are frames requiring different crops in the video, don't fill these up.  More on this at the end of this chapter.</p>
<p>To illustrate what a source requiring <code>FillBorders</code> might look like,
let's look at Parasite (2019)'s SDR UHD once again, which requires
an uneven crop of 277. However, we can't crop this due to chroma
subsampling, so we need to fill one row. To illustrate this, we'll
only be looking at the top rows. Cropping with respect to chroma
subsampling nets us:</p>
<pre><code class="language-py">crp = src.std.Crop(top=276)
</code></pre>
<p align="center">
<img src='Pictures/fb_src.png';"/>
</p>
<p>Obviously, we want to get rid of the black line at the top, so let's
use <code>FillBorders</code> on it:</p>
<pre><code class="language-py">fil = crp.fb.FillBorders(top=1, mode=&quot;fillmargins&quot;)
</code></pre>
<p align="center">
<img src='Pictures/fb_luma.png' onmouseover="this.src='Pictures/fb_src.png';" onmouseout="this.src='Pictures/fb_luma.png';"/>
</p>
<p>This already looks better, but the orange tones look washed out.
This is because <code>FillBorders</code> only fills one chroma if <strong>two</strong> luma
are fixed. So, we need to fill chroma as well. To make this easier
to write, let's use the <code>awsmfunc</code> wrapper:</p>
<pre><code class="language-py">fil = awf.fb(crp, top=1)
</code></pre>
<p align="center">
<img src='Pictures/fb_lumachroma.png' onmouseover="this.src='Pictures/fb_luma.png';" onmouseout="this.src='Pictures/fb_lumachroma.png';"/>
</p>
<p>Our source is now fixed. Some people may want to resize the chroma
to maintain original aspect ratio performing lossy resampling on chroma, but whether
this is the way to go is not generally agreed upon. If you want to go this route:</p>
<pre><code class="language-py">top = 1
bot = 1
new_height = crp.height - (top + bot)
fil = awf.fb(crp, top=top, bottom=bot)
out = fil.resize.Spline36(crp.width, new_height, src_height=new_height, src_top=top) 
</code></pre>
<details>
<summary>In-depth function explanation</summary>
<code>FillBorders</code> has four modes, although we only really care about mirror, fillmargins, and fixborders.
The mirror mode literally just mirrors the previous pixels.  Contrary to the third mode, repeat, it doesn't just mirror the final row, but the rows after that for fills greater than 1.  This means that, if you only fill one row, these modes are equivalent.  Afterwards, the difference becomes obvious.
<p>In fillmargins mode, it works a bit like a convolution, whereby for rows it does a [2, 3, 2] of the next row's pixels, meaning it takes 2 of the left pixel, 3 of the middle, and 2 of the right, then averages.  For borders, it works slightly differently: the leftmost pixel is just a mirror of the next pixel, while the eight rightmost pixels are also mirrors of the next pixel.  Nothing else happens here.</p>
<p>The fixborders mode is a modified fillmargins that works the same for rows and columns.  It compares fills with emphasis on the left, middle, and right with the next row to decide which one to use.</p>
</details>
<h2 id="continuityfixer"><a class="header" href="#continuityfixer"><code>ContinuityFixer</code></a></h2>
<p>From <a href="https://gitlab.com/Ututu/VS-ContinuityFixer"><code>cf</code></a>.  <code>ContinuityFixer</code> works by comparing the rows/columns specified to
the amount of rows/columns specified by <code>range</code> around it and
finding new values via least squares regression. Results are similar
to <code>bbmod</code>, but it creates entirely fake data, so it's preferable to
use <code>rektlvls</code> or <code>bbmod</code> with a high blur instead. Its settings
look as follows:</p>
<pre><code class="language-py">fix = core.cf.ContinuityFixer(src=clip, left=[0, 0, 0], right=[0, 0, 0], top=[0, 0, 0], bottom=[0, 0, 0], radius=1920)
</code></pre>
<p>This is assuming you're working with 1080p footage, as <code>radius</code>'s
value is set to the longest set possible as defined by the source's
resolution. I'd recommend a lower value, although not going much
lower than 3, as at that point, you may as well be copying pixels
(see <code>FillBorders</code> below for that). What will probably throw off
most newcomers is the array I've entered as the values for
rows/columns to be fixed. These denote the values to be applied to
the three planes. Usually, dirty lines will only occur on the luma
plane, so you can often leave the other two at a value of 0. Do note
an array is not necessary, so you can also just enter the amount of
rows/columns you'd like the fix to be applied to, and all planes
will be processed.</p>
<p>As <code>ContinuityFixer</code> is less likely to keep original data in tact, it's recommended to prioritize <code>bbmod</code> over it.</p>
<p>Let's look at the <code>bbmod</code> example again and apply <code>ContinuityFixer</code>:</p>
<pre><code class="language-py">fix = src.cf.ContinuityFixer(top=[6, 6, 6], radius=10)
</code></pre>
<p align="center">
<img src='Pictures/dirtfixes2.png' onmouseover="this.src='Pictures/dirtfixes0.png';" onmouseout="this.src='Pictures/dirtfixes2.png';"/>
</p>
<p>Let's compare this with the bbmod fix (remember to mouse-over to compare):</p>
<p align="center">
<img src='Pictures/dirtfixes2.png' onmouseover="this.src='Pictures/dirtfixes1.png';" onmouseout="this.src='Pictures/dirtfixes2.png';"/>
</p>
The result is ever so slightly in favor of <code>ContinuityFixer</code> here.
This will rarely be the case, as `ContinuityFixer` tends to be more destructive
than `bbmod` already is.
<p>Just like <code>bbmod</code>, <code>ContinuityFixer</code> shouldn't be used on more than two rows/columns.  Again, if you're resizing, you can change this maximum accordingly:
\[
max_\mathrm{resize} = max \times \frac{resolution_\mathrm{source}}{resolution_\mathrm{resized}}
\]</p>
<details>
<summary>In-depth function explanation</summary>
<code>ContinuityFixer</code> works by calculating the <a href=https://en.wikipedia.org/wiki/Least_squares>least squares
regression</a> of the pixels within the radius. As such, it creates
entirely fake data based on the image's likely edges.  No special explanation here.
</details>
<h2 id="referencefixer"><a class="header" href="#referencefixer"><code>ReferenceFixer</code></a></h2>
<p>From <a href="https://github.com/sekrit-twc/EdgeFixer"><code>edgefixer</code></a>.  This requires the original version of <code>edgefixer</code> (<code>cf</code> is just an
old port of it, but it's nicer to use and processing hasn't
changed). I've never found a good use for it, but in theory, it's
quite neat. It compares with a reference clip to adjust its edge fix
as in <code>ContinuityFixer</code>.:</p>
<pre><code class="language-py">fix = core.edgefixer.Reference(src, ref, left=0, right=0, top=0, bottom=0, radius = 1920)
</code></pre>
<h2 id="notes"><a class="header" href="#notes">Notes</a></h2>
<h3 id="too-many-rowscolumns"><a class="header" href="#too-many-rowscolumns">Too many rows/columns</a></h3>
<p>One thing that shouldn't be ignored is that applying these fixes (other
than <code>rektlvls</code>) to too many rows/columns may lead to these looking
blurry on the end result. Because of this, it's recommended to use
<code>rektlvls</code> whenever possible or carefully apply light fixes to only the
necessary rows. If this fails, it's better to try <code>bbmod</code> before using
<code>ContinuityFixer</code>.</p>
<h3 id="resizing"><a class="header" href="#resizing">Resizing</a></h3>
<p>It's important to note that you should <em>always</em> fix dirty lines before
resizing, as not doing so will introduce even more dirty lines. However,
it is important to note that, if you have a single black line at an edge
that you would use <code>FillBorders</code> on, you should remove that using your
resizer.</p>
<p>For example, to resize a clip with a single filled line at the top to
\(1280\times536\) from \(1920\times1080\):</p>
<pre><code class="language-py">top_crop = 138
bot_crop = 138
top_fill = 1
bot_fill = 0
src_height = src.height - (top_crop + bot_crop) - (top_fill + bot_fill)
crop = core.std.Crop(src, top=top_crop, bottom=bot_crop)
fix = core.fb.FillBorders(crop, top=top_fill, bottom=bot_fill, mode=&quot;fillmargins&quot;)
resize = core.resize.Spline36(1280, 536, src_top=top_fill, src_height=src_height)
</code></pre>
<h3 id="diagonal-borders"><a class="header" href="#diagonal-borders">Diagonal borders</a></h3>
<p>If you're dealing with diagonal borders, the proper approach here is to
mask the border area and merge the source with a <code>FillBorders</code> call. An
example of this (from the Your Name (2016)):</p>
<p align="center">
<img src='Pictures/improper_borders0.png' onmouseover="this.src='Pictures/improper_borders1.png';" onmouseout="this.src='Pictures/improper_borders0.png';"/>
</p>
<p>Fix compared with unmasked in fillmargins mode and contrast adjusted for clarity:</p>
<p align="center">
<img src='Pictures/improper_borders_adjusted1.png' onmouseover="this.src='Pictures/improper_borders_adjusted2.png';" onmouseout="this.src='Pictures/improper_borders_adjusted1.png';"/>
</p>
<p>Code used (note that this was detinted after):</p>
<pre><code class="language-py">mask = core.std.ShufflePlanes(src, 0, vs.GRAY).std.Binarize(43500)
cf = core.fb.FillBorders(src, top=6, mode=&quot;mirror&quot;).std.MaskedMerge(src, mask)
</code></pre>
<h3 id="finding-dirty-lines"><a class="header" href="#finding-dirty-lines">Finding dirty lines</a></h3>
<p>Dirty lines can be quite difficult to spot. If you don't immediately
spot any upon examining borders on random frames, chances are you'll be
fine. If you know there are frames with small black borders on each
side, you can use something like the <a href="https://gitlab.com/snippets/1834089">following script</a>:</p>
<pre><code class="language-py">def black_detect(clip, thresh=None):
    if thresh:
        clip = core.std.ShufflePlanes(clip, 0, vs.GRAY).std.Binarize(
            &quot;{0}&quot;.format(thresh)).std.Invert().std.Maximum().std.Inflate( ).std.Maximum().std.Inflate()
    l = core.std.Crop(clip, right=clip.width / 2)
    r = core.std.Crop(clip, left=clip.width / 2)
    clip = core.std.StackHorizontal([r, l])
    t = core.std.Crop(clip, top=clip.height / 2)
    b = core.std.Crop(clip, bottom=clip.height / 2)
    return core.std.StackVertical([t, b])
</code></pre>
<p>This script will make values under the threshold value (i.e. the black
borders) show up as vertical or horizontal white lines in the middle on
a mostly black background. If no threshold is given, it will simply
center the edges of the clip. You can just skim through your video with
this active. An automated alternative would be <a href="https://git.concertos.live/AHD/awsmfunc/src/branch/master/awsmfunc/detect.py"><code>dirtdtct</code></a>, which scans
the video for you.</p>
<p>Other kinds of variable dirty lines are a bitch to fix and require
checking scenes manually.</p>
<h3 id="variable-borders"><a class="header" href="#variable-borders">Variable borders</a></h3>
<p>An issue very similar to dirty lines is unwanted borders. During scenes with
different crops (e.g. IMAX or 4:3), the black borders may sometimes not
be entirely black, or be completely messed up. In order to fix this,
simply crop them and add them back. You may also want to fix dirty lines
that may have occurred along the way:</p>
<pre><code class="language-py">crop = core.std.Crop(src, left=100, right=100)
clean = core.cf.ContinuityFixer(crop, left=2, right=2, top=0, bottom=0, radius=25)
out = core.std.AddBorders(clean, left=100, right=100)
</code></pre>
<p>If you're resizing, you should crop these off before resizing, then add the borders back, as leaving the black bars in during the resize will create dirty lines:</p>
<pre><code class="language-py">crop = src.std.Crop(left=100, right=100)
clean = crop.cf.ContinuityFixer(left=2, right=2, top=2, radius=25)
resize = awf.zresize(clean, preset=720)
border_size = (1280 - resize.width) / 2
bsize_mod2 = border_size % 2
out = resize.std.AddBorders(left=border_size - bsize_mod2, right=border_size + bsize_mod2)
</code></pre>
<p>In the above example, we have to add more to one side than the other to reach our desired width.  Ideally, your <code>border_size</code> will be mod2 and you won't have to do this.</p>
<p>If you know you have borders like these, you can use <code>brdrdtct</code> from <code>awsmfunc</code> similarly to <code>dirtdtct</code> to scan the file for them.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h2 id="088伽马的错误"><a class="header" href="#088伽马的错误">0.88伽马的错误</a></h2>
<p>如果你有两个信号源，其中一个明显比另一个亮，那么你的亮的信号源有可能是受到了所谓的伽马缺陷的影响。
如果是这种情况，请执行以下操作（针对16位），看看是否能解决这个问题:</p>
<pre><code class="language-py">out = core.std.Levels(src, gamma=0.88, min_in=4096, max_in=60160, min_out=4096, max_out=60160, planes=0)
</code></pre>
<p align="center">
<img src='Pictures/gamma_before.png' onmouseover="this.src='Pictures/gamma_after.png';" onmouseout="this.src='Pictures/gamma_before.png';" />
</p>
<p>不要在低位深度下执行这一操作。较低的比特深度可能并将导致色带。</p>
<p align="center">
<img src='Pictures/gamma_lbd.png' onmouseover="this.src='Pictures/gamma_hbd.png';" onmouseout="this.src='Pictures/gamma_lbd.png';" />
</p>
<details>
<summary>深入解释</summary>
这个错误似乎源于苹果软件。 <a href="https://vitrolite.wordpress.com/2010/12/31/quicktime_gamma_bug/">这篇博文</a>是人们在网上可以找到的关于这个错误的少数提及之一。
<p>其原因可能是软件不必要地试图在NTSC伽马（2.2）和PC伽马（2.5）之间进行转换，因为 \(\frac{2.2}{2.5}=0.88\).</p>
<p>为了解决这个问题，每个值都必须提高到0.88的幂，尽管必须进行电视范围标准化:</p>
<p>\[
v_\mathrm{new} = \left( \frac{v - min_\mathrm{in}}{max_\mathrm{in} - min_\mathrm{in}} \right) ^ {0.88} \times (max_\mathrm{out} - min_\mathrm{out}) + min_\mathrm{out}
\]</p>
<p>对于那些好奇伽马虫源和源会有什么不同的人来说：除了16、232、233、234和235以外的所有数值都是不同的，最大和最常见的差异是10，从63持续到125。
由于可以打出同等数量的数值，而且该操作通常是在高比特深度下进行的，因此显著的细节损失是不太可能的。
然而，请注意，无论比特深度如何，这都是一个有损失的过程。</p>
</details>
<p>你也可以使用<code>awsmfunc</code>中的<code>fixlvls</code>封装库，在32位精度下轻松完成这个任务。</p>
<h2 id="双重范围压缩"><a class="header" href="#双重范围压缩">双重范围压缩</a></h2>
<p>一个类似的问题是双范围压缩。 当这种情况发生时，luma值会在30和218之间。 这可以通过以下方法轻松解决:</p>
<pre><code class="language-py">out = src.resize.Point(range_in=0, range=1, dither_type=&quot;error_diffusion&quot;)
out = out.std.SetFrameProp(prop=&quot;_ColorRange&quot;, intval=1)
</code></pre>
<p align="center">
<img src='Pictures/double_range_compression0.png' onmouseover="this.src='Pictures/double_range_compression1.png';" onmouseout="this.src='Pictures/double_range_compression0.png';" />
</p>
<details>
<summary>深入解释</summary>
这个问题意味着在编码过程中，某些东西或某些人假定输入是全范围的，尽管它已经是有限范围的。 由于最终的结果通常是有限的范围，这个感知的问题被 "修复 "了。
<p>实际上，我们也可以在<code>std.Levels</code>中做完全相同的事情。 以下是应用于改变范围的数学方法:</p>
<p>\[
v_\mathrm{new} = \left( \frac{v - min_\mathrm{in}}{max_\mathrm{in} - min_\mathrm{in}} \right) \times (max_\mathrm{out} - min_\mathrm{out}) + min_\mathrm{out}
\]</p>
<p>对于范围压缩，使用以下数值:
\[
min_\mathrm{in} = 0 \qquad max_\mathrm{in} = 255 \qquad min_\mathrm{out} = 16 \qquad max_\mathrm{out} = 235
\]</p>
<p>由于zlib调整器在内部使用32位精度来执行这个，所以最简单的就是使用这些。 然而，这些将改变文件的<code>_ColorRange</code>属性，因此需要使用<code>std.SetFrameProp</code>:</p>
</details>
<h2 id="其他不正确的层级"><a class="header" href="#其他不正确的层级">其他不正确的层级</a></h2>
<p>一个密切相关的问题是其他不正确的层级。要解决这个问题，最好是使用一个具有正确层级的参考源，找到与16和235相当的值，然后从那里进行调整（为了清楚起见，在更高的比特深度中这样做）:</p>
<pre><code class="language-py">out = src.std.Levels(min_in=x, min_out=16, max_in=y, max_out=235)
</code></pre>
<p>然而，这通常是不可能的。 相反，我们可以做以下数学运算来计算出正确的调整值:
\[
v = \frac{v_\mathrm{new} - min_\mathrm{out}}{max_\mathrm{out} - min_\mathrm{out}} \times (max_\mathrm{in} - min_\mathrm{in}) + min_\mathrm{in}
\]</p>
<p>因此，我们可以从要调整的源中选择任何低值，将其设置为 \(min_\mathrm{in}\)，在参考源中选择同一像素的值作为 \(min_\mathrm{out}\)。对于高值和最大值，我们也是这样做的。然后, 我们用16和235来计算 (再次，最好是高位深度--16位的4096和60160，32位浮动的0和1等等。) 这个 \(v_\mathrm{new}\) ，输出值将是上面VapourSynth代码中我们的 \(x\) 和 \(y\)。</p>
<p>为了说明这一点，让我们使用《燃烧》（2018）的德国和美国蓝光片。 美国的蓝光有正确的水平，而德国的蓝光有不正确的水平。</p>
<p align="center">
<img src='Pictures/burning_usa0.png' onmouseover="this.src='Pictures/burning_ger0.png';" onmouseout="this.src='Pictures/burning_usa0.png';" />
</p>
<p>这里德国的高值是199，而美国的相同像素是207。 对于低值，我们可以找到29和27。 通过这些，我们得到18.6和225.4。 对更多的像素和不同的帧进行这些操作，然后取其平均值，我们得到19和224。 用这些值来调整luma，使我们更接近参考视频的<sup class="footnote-reference"><a href="#1">1</a></sup>。</p>
<p align="center">
<img src='Pictures/burning_ger_fixed0.png' onmouseover="this.src='Pictures/burning_usa_fixed0.png';" onmouseout="this.src='Pictures/burning_ger_fixed0.png';" />
</p>
<details>
<summary>深入解释</summary>
看过前面解释的人应该认识到这个函数，因为它是用于水平调整的函数的逆向函数。 我们只需把它反过来，把我们的期望值设为 \(v_\mathrm{new}\)并进行计算。
</details>
<h2 id="不恰当的颜色矩阵"><a class="header" href="#不恰当的颜色矩阵">不恰当的颜色矩阵</a></h2>
<p>如果你有一个不恰当的颜色矩阵的信号源，你可以用以下方法解决这个问题
用以下方法解决。</p>
<pre><code class="language-py">out = core.resize.Point(src, matrix_in_s='470bg', matrix_s='709')
</code></pre>
<p><code>'470bg'</code>也就是被称为601的东西。要知道你是否应该这样做，你需要一些参考来源，最好不是网络来源。从技术上讲，你可以识别坏的颜色，并意识到有必要改变矩阵，但在这种情况下，人们应该非常确定。</p>
<p align="center">
<img src='Pictures/burning_matrix_before.png' onmouseover="this.src='Pictures/burning_matrix_after.png';" onmouseout="this.src='Pictures/burning_matrix_before.png';" />
</p>
<details>
<summary>深入解释</summary>
颜色矩阵定义了YCbCr和RGB之间的转换是如何进行的。 由于RGB自然没有任何子采样，剪辑首先从4:2:0转换为4:4:4，然后从YCbCr转换为RGB，然后再进行还原。 在YCbCr到RGB的转换过程中，我们假设是Rec.601矩阵系数，而在转换回来的过程中，我们指定是Rec.709。
<p>之所以很难知道是否假设了不正确的标准，是因为两者涵盖了CIE 1931的类似范围。 色度图应使这一点很明显（包括Rec.2020作为参考）:</p>
<p align="center">
<img src='Pictures/colorspaces.svg'/>
</p>
</details>
<h2 id="四舍五入错误"><a class="header" href="#四舍五入错误">四舍五入错误</a></h2>
<p>一个轻微的绿色色调可能表明发生了舍入错误。
为了解决这个问题，我们需要在比源文件更高的比特深度上增加半步。</p>
<pre><code class="language-py">high_depth = vsutil.depth(src, 16)
half_step = high_depth.std.Expr(&quot;x 128 +&quot;)
out = vsutil.depth(half_step, 8)
</code></pre>
<p align="center">
<img src='Pictures/rounding_0.png' onmouseover="this.src='Pictures/rounding_1.png';" onmouseout="this.src='Pictures/rounding_0.png';" />
</p>
<p>另外，可以使用<a href="https://github.com/Irrational-Encoding-Wizardry/lvsfunc"><code>lvsfunc.misc.fix_cr_tint</code></a>代替。
它的默认值与上述内容相当。</p>
<details>
<summary>深入解释</summary>
当工作室从他们的10位母版变成8位时，他们的软件可能总是向下四舍五入（例如1.9会被四舍五入为1）。
我们解决这个问题的方法只是增加了一个8位的半步，如（0.5乘以2 ^ {16 - 8} = 128\）
</details>
<h2 id="除着色"><a class="header" href="#除着色">除着色</a></h2>
<p>请注意，只有在其他方法都失败的情况下，你才应该采用这种方法。</p>
<p>如果你有一个较好的带色调的源，和一个较差的不带色调的源，你想把它去掉，你可以通过<a href="https://github.com/sekrit-twc/timecube"><code>timecube</code></a>和<a href="https://valeyard.net/2017/03/drdres-color-matching-tool-v1-2.php">DrDre's Color Matching Tool</a><sup class="footnote-reference"><a href="#2">2</a></sup>来实现。首先，在该工具中添加两张参考截图，导出LUT，保存它，并通过类似的方式添加它:</p>
<pre><code class="language-py">clip = core.resize.Point(src, matrix_in_s=&quot;709&quot;, format=vs.RGBS)
detint = core.timecube.Cube(clip, &quot;LUT.cube&quot;)
out = core.resize.Point(detint, matrix=1, format=vs.YUV420P16, dither_type=&quot;error_diffusion&quot;)
</code></pre>
<p align="center">
<img src='Pictures/detint_before2.png' onmouseover="this.src='Pictures/detint_after2.png';" onmouseout="this.src='Pictures/detint_before2.png';" />
</p>
<div class="footnote-definition" id="1"><sup class="footnote-definition-label">1</sup>
<p>为了简单起见，这里没有触及色度平面。 这些需要做的工作远远多于luma平面，因为很难找到非常鲜艳的颜色，尤其是像这样的截图。</p>
</div>
<div class="footnote-definition" id="2"><sup class="footnote-definition-label">2</sup>
<p>令人遗憾的是，这个程序是闭源的。 我不知道有什么替代品。</p>
</div>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="this-needs-to-be-reformatted"><a class="header" href="#this-needs-to-be-reformatted">this needs to be reformatted</a></h1>
<p>Masking is a less straightforward topic. The idea is to limit the
application of filters according to the source image's properties. A
mask will typically be grayscale, whereby how much of the two clips in
question are applied is determined by the mask's brightness. So, if you
do</p>
<pre><code class="language-py">mask = mask_function(src)
filtered = filter_function(src)
merge = core.std.MaskedMerge(src, filtered, mask)
</code></pre>
<p>The <code>filtered</code> clip will be used for every completely white pixel in
<code>mask</code>, and the <code>src</code> clip for every black pixel, with in-between values
determining the ratio of which clip is applied. Typically, a mask will
be constructed using one of the following three functions:</p>
<ul>
<li>
<p><code>std.Binarize</code>: This simply separates pixels by whether they are
above or below a threshold and sets them to black or white
accordingly.</p>
</li>
<li>
<p><code>std.Expr</code>: Known to be a very complicated function. Applies logic
via reverse Polish notation. If you don't know what this is, read up
on Wikipedia. Some cool things you can do with this are make some
pixels brighter while keeping others the same (instead of making
them dark as you would with <code>std.Binarize</code>):
<code>std.Expr(&quot;x 2000 &gt; x 10 * x ?&quot;)</code>. This would multiply every value
above 2000 by ten and leave the others be. One nice use case is for
in between values:
<code>std.Expr(&quot;x 10000 &gt; x 15000 &lt; and x {} = x 0 = ?&quot;.format(2**src.format.bits_per_sample - 1))</code>.<br />
This makes every value between 10 000 and 15 000 the maximum value
allowed by the bit depth and makes the rest zero, just like how a
<code>std.Binarize</code> mask would. Many other functions can be performed via
this.</p>
</li>
<li>
<p><code>std.Convolution</code>: In essence, apply a matrix to your pixels. The
documentation explains it well, so just read that if you don't get
it. Lots of masks are defined via convolution kernels. You can use
this to do a whole lot of stuff. For example, if you want to average
all the values surrounding a pixel, do
<code>std.Convolution([1, 1, 1, 1, 0, 1, 1, 1, 1])</code>. To illustrate, let's
say you have a pixel with the value \(\mathbf{1}\) with the following
\(3\times3\) neighborhood:</p>
<p>\[\begin{bmatrix}
0 &amp; 2 &amp; 4 \\
6 &amp; \mathbf{1} &amp; 8 \\
6 &amp; 4 &amp; 2
\end{bmatrix}\]</p>
<p>Now, let's apply a convolution kernel:</p>
<p>\[\begin{bmatrix}
2 &amp; 1 &amp; 3 \\
1 &amp; 0 &amp; 1 \\
4 &amp; 1 &amp; 5
\end{bmatrix}\]</p>
<p>This will result in the pixel 1 becoming:
\[\frac{1}{18} \times (2 \times 0 + 1 \times 2 + 3 \times 4 + 1 \times 6 + 0 \times \mathbf{1} + 1 \times 8 + 4 \times 6 + 1 \times 4 + 5 \times 2) = \frac{74}{18} \approx 4\]</p>
</li>
</ul>
<p>So, let's say you want to perform what is commonly referred to as a
simple &quot;luma mask&quot;:</p>
<pre><code class="language-py">y = core.std.ShufflePlanes(src, 0, vs.GRAY)
mask = core.std.Binarize(y, 5000)
merge = core.std.MaskedMerge(filtered, src, mask)
</code></pre>
<p>In this case, I'm assuming we're working in 16-bit. What <code>std.Binarize</code>
is doing here is making every value under 5000 the lowest and every
value above 5000 the maximum value allowed by our bit depth. This means
that every pixel above 5000 will be copied from the source clip.</p>
<p>Let's try this using a <code>filtered</code> clip which has every pixel's value
multiplied by 8:</p>
<p><img src="filtering/Pictures/luma_mask.png" alt="Binarize mask applied to luma with filtered clip being std.Expr(&quot;x 8 *&quot;)." /></p>
<p>Simple binarize masks on luma are very straightforward and often do a
good job of limiting a filter to the desired area, especially as dark
areas are more prone to banding and blocking.</p>
<p>A more sophisticated version of this is <code>adaptive_grain</code> from earlier in
this guide. It scales values from black to white based on both the
pixel's luma value compared to the image's average luma value. A more
in-depth explanation can be found on <a href="https://blog.kageru.moe/legacy/adaptivegrain.html">the creator's blog</a>. We
manipulate this mask using a <code>luma_scaling</code> parameter. Let's use a very
high value of 500 here:</p>
<p><img src="filtering/Pictures/adg_mask.png" alt="kgf.adaptive_grain(y, show_mask=True, luma_scaling=500) mask applied to luma with filtered clip being std.Expr(&quot;x 8 *&quot;)." /></p>
<p>Alternatively, we can use an <code>std.Expr</code> to merge the clips via the
following logic:</p>
<pre><code>if abs(src - filtered) &lt;= 1000:
    return filtered
elif abs(src - filtered) &gt;= 30000:
    return src
else:
    return src + (src - filtered) * (30000 - abs(src - filtered)) / 29000
</code></pre>
<p>This is almost the exact algorithm used in <code>mvsfunc.LimitFilter</code>, which
<code>GradFun3</code> uses to apply its bilateral filter. In VapourSynth, this
would be:</p>
<pre><code class="language-py">expr = core.std.Expr([src, filtered], &quot;x y - abs 1000 &gt; x y - abs 30000 &gt; x x y - 30000 x y - abs - * 29000 / + x ? y ?&quot;)
</code></pre>
<p><img src="filtering/Pictures/expr_limit.png" alt="LimitFilter style expression to apply filter std.Expr(&quot;x 8 *&quot;) to source." /></p>
<p>Now, let's move on to the third option: convolutions, or more
interestingly for us, edge masks. Let's say you have a filter that
smudges details in your clip, but you still want to apply it to
detail-free areas. We can use the following convolutions to locate
horizontal and vertical edges in the image:</p>
<p>\[\begin{aligned}
&amp;\begin{bmatrix}
1 &amp; 0 &amp; -1 \\
2 &amp; 0 &amp; -2 \\
1 &amp; 0 &amp; -1
\end{bmatrix}
&amp;\begin{bmatrix}
1 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; -2 &amp; -1
\end{bmatrix}\end{aligned}\]</p>
<p>Combining these two is what is commonly referred to as a Sobel-type edge
mask. It produces the following for our image of the lion:</p>
<p><img src="filtering/Pictures/sobel.png" alt="image" />
Now, this result is obviously rather boring. One can see a rough outline
of the background and the top of the lion, but not much more can be made
out.<br />
To change this, let's introduce some new functions:</p>
<ul>
<li>
<p><code>std.Maximum/Minimum</code>: Use this to grow or shrink your mask, you may
additionally want to apply <code>coordinates=[0, 1, 2, 3, 4, 5, 6, 7]</code>
with whatever numbers work for you in order to specify weights of
the surrounding pixels.</p>
</li>
<li>
<p><code>std.Inflate/Deflate</code>: Similar to the previous functions, but
instead of applying the maximum of pixels, it merges them, which
gets you a slight blur of edges. Useful at the end of most masks so
you get a slight transition between masked areas.</p>
</li>
</ul>
<p>We can combine these with the <code>std.Binarize</code> function from before to get
a nifty output:</p>
<pre><code class="language-py">mask = y.std.Sobel()
binarize = mask.std.Binarize(3000)
maximum = binarize.std.Maximum().std.Maximum()
inflate = maximum.std.Inflate().std.Inflate().std.Inflate()
</code></pre>
<p><img src="filtering/Pictures/sobel_manipulated.png" alt="Sobel mask from before manipulated with std.Binarize, std.Maximum, and std.Inflate." /></p>
<p>A common example of a filter that might smudge the output is an
anti-aliasing or a debanding filter. In the case of an anti-aliasing
filter, we apply the filter via the mask to the source, while in the
case of the debander, we apply the source via the mask to the filtered
source:</p>
<pre><code class="language-py">mask = y.std.Sobel()

aa = taa.TAAmbk(src, aatype=3, mtype=0)
merge = core.std.MaskedMerge(src, aa, mask)

deband = src.f3kdb.Deband()
merge = core.std.MaskedMerge(deband, src, mask)
</code></pre>
<p>We can also use a different edge mask, namely <code>kgf.retinex_edgemask</code>,
which raises contrast in dark areas and creates a second edge mask using
the output of that, then merges it with the edge mask produced using the
untouched image:</p>
<p><img src="filtering/Pictures/retinex_edgemask.png" alt="kgf.retinex_edgemask applied to luma." /></p>
<p>This already looks great. Let's manipulate it similarly to before and
see how it affects a destructive deband in the twig area at the bottom:</p>
<pre><code class="language-py">deband = src.f3kdb.Deband(y=150, cb=150, cr=150, grainy=0, grainc=0)
mask = kgf.retinex_edgemask(src).std.Binarize(8000).std.Maximum()
merge = core.std.MaskedMerge(deband, src, mask)
</code></pre>
<p><img src="filtering/Pictures/masked_deband.png" alt="A very strong deband protected using kgf.retinex_edgemask." /></p>
<p>While some details remain smudged, we've successfully recovered a very
noticeable portion of the twigs. Another example of a deband suffering
from detail loss without an edge mask can be found under figure
<a href="filtering/masking.html#fig:18">35</a> in the appendix.</p>
<p>Other noteworthy edge masks easily available in VapourSynth include:</p>
<ul>
<li>
<p><code>std.Prewitt</code> is similar to Sobel. It's the same operator with the 2
switched out for a 1.</p>
</li>
<li>
<p><code>tcanny.TCanny</code> is basically a Sobel mask thrown over a blurred
clip.</p>
</li>
<li>
<p><code>kgf.kirsch</code> will generate almost identical results to
<code>retinex_edgemask</code> in bright scenes, as it's one of its components.
Slower than the others, as it uses more directions, but will get you
great results.</p>
</li>
</ul>
<p>Some edge mask comparisons can be found in the appendix under figures
<a href="filtering/masking.html#fig:16">26</a>{reference-type=&quot;ref&quot; reference=&quot;fig:16&quot;},
<a href="filtering/masking.html#fig:10">30</a>{reference-type=&quot;ref&quot; reference=&quot;fig:10&quot;} and
<a href="filtering/masking.html#fig:23">34</a>{reference-type=&quot;ref&quot; reference=&quot;fig:23&quot;}.</p>
<p>As a debanding alternative to edge masks, we can also use &quot;range&quot;
masks, which employ <code>std.Minimum</code> and <code>std.Maximum</code> to locate details.
The most well known example of this is the mask inside <code>GradFun3</code>. This
works as follows:</p>
<p>Then, two clips are created, one which will employ <code>std.Maximum</code>, while
the other obviously will use <code>std.Minimum</code>. These use special
coordinates depending on the <code>mrad</code> value given. If
\(\mathtt{mrad} \mod 3 = 1\), <code>[0, 1, 0, 1, 1, 0, 1, 0]</code> will be used as
coordinates. Otherwise, <code>[1, 1, 1, 1, 1, 1, 1, 1]</code> is used. Then, this
process is repeated with \(\mathtt{mrad} = \mathtt{mrad} - 1\) until
$\mathtt{mrad} = 0$. This all probably sounds a bit overwhelming, but
it's really just finding the maximum and minimum values for each pixel
neighborhood.</p>
<p>Once these are calculated, the minimized mask is subtracted from the
maximized mask, and the mask is complete. So, let's look at the output
compared to the modified <code>retinex_edgemask</code> from earlier:</p>
<p><img src="filtering/Pictures/gradfun3_mask.png" alt="Comparison of retinex_edgemask.std.Binarize(8000).std.Maximum() and default GradFun3." /></p>
<p>Here, we get some more pixels picked up by the <code>GradFun3</code> mask in the
skies and some brighter flat textures. However, the retinex-type edge
mask prevails in darker, more detailed areas. Computationally, our
detail mask is a lot quicker, however, and it does pick up a lot of what
we want, so it's not a bad choice.</p>
<p>Fortunately for us, this isn't the end of these kinds of masks. There
are two notable masks based on this concept: <a href="https://pastebin.com/SHQZjVJ5"><code>debandmask</code></a> and
<code>lvsfunc.denoise.detail_mask</code>. The former takes our <code>GradFun3</code> mask and
binarizes it according to the input luma's brightness. Four parameters
play a role in this process: <code>lo</code>, <code>hi</code>, <code>lothr</code>, and <code>hithr</code>. Values
below <code>lo</code> are binarized according to <code>lothr</code>, values above <code>hi</code> are
binarized according to <code>hithr</code>, and values in between are binarized
according to a linear scaling between the two thresholds:</p>
<p>\[\frac{\mathtt{mask} - \mathtt{lo}}{\mathtt{hi} - \mathtt{lo}} \times (\mathtt{hithr} - \mathtt{lothr}) + \mathtt{lothr}\]</p>
<p>This makes it more useful in our specific scenario, as the mask becomes
stronger in darks compared to <code>GradFun3</code>. When playing around with the
parameters, we can e.. lower <code>lo</code> so we our very dark areas aren't
affected too badly, lower <code>lothr</code> to make it stronger in these darks,
raise <code>hi</code> to enlarge our <code>lo</code> to <code>hi</code> gap, and raise <code>hithr</code> to weaken
it in brights. Simple values might be
<code>lo=22 &lt;&lt; 8, lothr=250, hi=48 &lt;&lt; 8, hithr=500</code>:</p>
<p><img src="filtering/Pictures/debandmask_comparison.png" alt="Comparison of retinex_edgemask.std.Binarize(8000).std.Maximum(), default GradFun3, and default debandmask(lo=22 &lt;&lt; 8, lothr=250, hi=48 &lt;&lt; 8, hithr=500)." /></p>
<p>While not perfect, as this is a tough scene, and parameters might not be
optimal, the difference in darks is obvious, and less banding is picked
up in the background's banding.</p>
<p>Our other option for an altered <code>GradFun3</code> is <code>lvf.denoise.detail_mask</code>.
This mask combines the previous idea of the <code>GradFun3</code> mask with a
Prewitt-type edge mask.</p>
<p>First, two denoised clips are created using <code>KNLMeansCL</code>, one with half
the other's denoise strength. The stronger one has a <code>GradFun3</code>-type
mask applied, which is then binarized, while the latter has a Prewitt
edge mask applied, which again is binarized. The two are then combined
so the former mask gets any edges it may have missed from the latter
mask.</p>
<p>The output is then put through two calls of <code>RemoveGrain</code>, the first one
setting each pixel to the nearest value of its four surrounding pixel
pairs' (e.. top and bottom surrounding pixels make up one pair) highest
and lowest average value. The second call effectively performs the
following convolution: 
\[\begin{bmatrix}
1 &amp; 2 &amp; 1 \\
2 &amp; 4 &amp; 2 \\
1 &amp; 2 &amp; 1
\end{bmatrix}\]</p>
<p>By default, the denoiser is turned off, but this is one of its
advantages for us in this case, as we'd like the sky to have fewer
pixels picked up while we'd prefer more of the rest of the image to be
picked up. To compare, I've used a binarize threshold similar to the one
used in the <code>debandmask</code> example. Keep in mind this is a newer mask, so
my inexperience with it might show to those who have played around with
it more:</p>
<p><img src="filtering/Pictures/detail_mask.png" alt="Comparison of retinex_edgemask.std.Binarize(8000).std.Maximum(), default GradFun3, default debandmask(lo=22 &lt;&lt; 8, lothr=250, hi=48 &lt;&lt; 8, hithr=500), and detail_mask(pre_denoise=.3, brz_a=300, brz_b=300)." /></p>
<p>Although an improvement in some areas, in this case, we aren't quite
getting the step up we would like. Again, better optimized parameters
might have helped.</p>
<p>In case someone wants to play around with the image used here, it's
available in this guide's repository:
<a href="https://git.concertos.live/Encode_Guide/mdbook-guide/src/branch/master/src/filtering/Pictures/lion.png">https://git.concertos.live/Encode_Guide/mdbook-guide/src/branch/master/src/filtering/Pictures/lion.png</a>.</p>
<p>Additionally, the following functions can be of help when masking,
limiting et cetera:</p>
<ul>
<li>
<p><code>std.MakeDiff</code> and <code>std.MergeDiff</code>: These should be
self-explanatory. Use cases can be applying something to a degrained
clip and then merging the clip back, as was elaborated in the
Denoising section.</p>
</li>
<li>
<p><code>std.Transpose</code>: Transpose (i.. flip) your clip.</p>
</li>
<li>
<p><code>std.Turn180</code>: Turns by 180 degrees.</p>
</li>
<li>
<p><code>std.BlankClip</code>: Just a frame of a solid color. You can use this to
replace bad backgrounds or for cases where you've added grain to an
entire movie but you don't want the end credits to be full of grain.
To maintain TV range, you can use
<code>std.BlankClip(src, color=[16, 128, 128]</code>) for 8-bit black. Also
useful for making area based masks.</p>
</li>
<li>
<p><code>std.Invert</code>: Self-explanatory. You can also just swap which clip
gets merged via the mask instead of doing this.</p>
</li>
<li>
<p><code>std.Limiter</code>: You can use this to limit pixels to certain values.
Useful for maintaining TV range (<code>std.Limiter(min=16, max=235)</code>).</p>
</li>
<li>
<p><code>std.Median</code>: This replaces each pixel with the median value in its
neighborhood. Mostly useless.</p>
</li>
<li>
<p><code>std.StackHorizontal</code>/<code>std.StackVertical</code>: Stack clips on top
of/next to each other.</p>
</li>
<li>
<p><code>std.Merge</code>: This lets you merge two clips with given weights. A
weight of 0 will return the first clip, while 1 will return the
second. The first thing you give it is a list of clips, and the
second item is a list of weights for each plane. Here's how to merge
chroma from the second clip into luma from the first:
<code>std.Merge([first, second], [0, 1])</code>. If no third value is given,
the second one is copied for the third plane.</p>
</li>
<li>
<p><code>std.ShufflePlanes</code>: Extract or merge planes from a clip. For
example, you can get the luma plane with
<code>std.ShufflePlanes(src, 0, vs.GRAY)</code>.</p>
</li>
</ul>
<p>If you want to apply something to only a certain area, you can use the
wrapper <a href="https://gitlab.com/Ututu/rekt"><code>rekt</code></a> or <code>rekt_fast</code>. The latter only applies you function
to the given area, which speeds it up and is quite useful for
anti-aliasing and similar slow filters. Some wrappers around this exist
already, like <code>rektaa</code> for anti-aliasing. Functions in <code>rekt_fast</code> are
applied via a lambda function, so instead of <code>src.f3kdb.Deband()</code>, you
input <code>rekt_fast(src, lambda x: x.f3kdb.Deband())</code>.</p>
<p>One more very special function is <code>std.FrameEval</code>. What this allows you
to do is evaluate every frame of a clip and apply a frame-specific
function. This is quite confusing, but there are some nice examples in
VapourSynth's documentation:
<a href="http://www.vapoursynth.com/doc/functions/frameeval.html">http://www.vapoursynth.com/doc/functions/frameeval.html</a>. Now, unless
you're interested in writing a function that requires this, you likely
won't ever use it. However, many functions use it, including<br />
<code>kgf.adaptive_grain</code>, <code>awf.FrameInfo</code>, <code>fvf.AutoDeblock</code>, <code>TAAmbk</code>, and
many more. One example I can think of to showcase this is applying a
different debander depending on frame type:</p>
<pre><code class="language-py">import functools
def FrameTypeDeband(n, f, clip):
    if clip.props['_PictType'].decode() == &quot;B&quot;:
        return core.f3kdb.Deband(clip, y=64, cr=0, cb=0, grainy=64, grainc=0, keep_tv_range=True, dynamic_grain=False)
    elif clip.props['_PictType'].decode() == &quot;P&quot;:
        return core.f3kdb.Deband(clip, y=48, cr=0, cb=0, grainy=64, grainc=0, keep_tv_range=True, dynamic_grain=False)
    else:
        return core.f3kdb.Deband(clip, y=32, cr=0, cb=0, grainy=64, grainc=0, keep_tv_range=True, dynamic_grain=False)
        
out = core.std.FrameEval(src, functools.partial(FrameTypeDeband, clip=src), src)
</code></pre>
<p>If you'd like to learn more, I'd suggest reading through the Irrational
Encoding Wizardry GitHub group's guide:
<a href="https://guide.encode.moe/encoding/masking-limiting-etc.html">https://guide.encode.moe/encoding/masking-limiting-etc.html</a> and
reading through most of your favorite Python functions for VapourSynth.
Pretty much all of the good ones should use some mask or have developed
their own mask for their specific use case.</p>
<p>Edge detection is also very thoroughly explained in a lot of digital
image processing textbooks, e.g. Digital Image Processing by Gonzalez and Woods.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="hqderingmod"><a class="header" href="#hqderingmod">HQDeringmod</a></h1>
<h1 id="sharpening"><a class="header" href="#sharpening">Sharpening</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h2 id="dehalo_alpha"><a class="header" href="#dehalo_alpha">DeHalo_alpha</a></h2>
<details>
<summary>old function explanation</summary>
`DeHalo_alpha` works by downscaling the source according to `rx` and
`ry` with a mitchell bicubic ($b=\nicefrac{1}{3},\ c=\nicefrac{1}{3}$)
kernel, scaling back to source resolution with blurred bicubic, and
checking the difference between a minimum and maximum (check
[3.2.14](#masking){reference-type="ref" reference="masking"} if you
don't know what this means) for both the source and resized clip. The
result is then evaluated to a mask according to the following
expressions, where $y$ is the maximum and minimum call that works on the
source, $x$ is the resized source with maximum and minimum, and
everything is scaled to 8-bit:
$$\texttt{mask} = \frac{y - x}{y + 0.0001} \times \left[255 - \texttt{lowsens} \times \left(\frac{y + 256}{512} + \frac{\texttt{highsens}}{100}\right)\right]$$
This mask is used to merge the source back into the resized source. Now,
the smaller value of each pixel is taken for a lanczos resize to
$(\texttt{height} \times \texttt{ss})\times(\texttt{width} \times \texttt{ss})$
of the source and a maximum of the merged clip resized to the same
resolution with a mitchell kernel. The result of this is evaluated along
with the minimum of the merged clip resized to the aforementioned
resolution with a mitchell kernel to find the minimum of each pixel in
these two clips. This is then resized to the original resolution via a
lanczos resize, and the result is merged into the source via the
following:
<pre><code>if original &lt; processed
    x - (x - y) * darkstr
else
    x - (x - y) * brightstr
</code></pre>
</details>
<h3 id="masking"><a class="header" href="#masking">Masking</a></h3>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="denoising"><a class="header" href="#denoising">Denoising</a></h1>
<h2 id="bm3d"><a class="header" href="#bm3d">BM3D</a></h2>
<h2 id="knlmeanscl"><a class="header" href="#knlmeanscl">KNLMeansCL</a></h2>
<h2 id="smdegrain"><a class="header" href="#smdegrain">SMDegrain</a></h2>
<h1 id="grain-dampening"><a class="header" href="#grain-dampening">Grain Dampening</a></h1>
<h2 id="stpresso"><a class="header" href="#stpresso">STPresso</a></h2>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="dehardsubbing"><a class="header" href="#dehardsubbing">Dehardsubbing</a></h1>
<h2 id="hardsubmask"><a class="header" href="#hardsubmask">hardsubmask</a></h2>
<h3 id="hardsubmask_fades"><a class="header" href="#hardsubmask_fades">hardsubmask_fades</a></h3>
<h1 id="delogoing"><a class="header" href="#delogoing">Delogoing</a></h1>
<h2 id="delogohd"><a class="header" href="#delogohd">DeLogoHD</a></h2>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><p>首先，您需要选择视频文件的一个较小区域作为参考，因为对整个内容进行测试将花费很长时间。
推荐的方法是使用 <code>awsmfunc</code>里的<code>SelectRangeEvery</code>:</p>
<pre><code>import awsmfunc as awf
out = awf.SelectRangeEvery(clip, every=15000, length=250, offset=[1000, 5000])
</code></pre>
<p>这里，第一个数字是节之间的偏移量，第二个数字是每个节的长度，偏移数组是从开始到结束的偏移量。</p>
<p>您需要使用相当长的剪辑（通常为几千帧），其中包括黑暗、明亮、静态和动作场景，
但是，它们的分布应该与它们在整个视频中的分布大致相同。</p>
<p>在测试设置时，您应该始终使用 2-pass 编码，因为许多设置会显着改变 CRF 为您提供的比特率。
对于最终编码，两者都很好，尽管 CRF 更快。</p>
<p>要找出最佳设置，请将它们相互对比并与源进行比较。 你可以单独这样做，或者在`awsmfunc'文件夹中交错排列一个文件夹。你通常也想给它们贴上标签，这样你就可以让你真正知道你在看哪个片段。</p>
<pre><code># Load the files before this
src = awf.FrameInfo(src, &quot;Source&quot;)
test1 = awf.FrameInfo(test1, &quot;Test 1&quot;)
test2 = awf.FrameInfo(test2, &quot;Test 2&quot;)
out = core.std.Interleave([src, test1, test2])

# You can also place them all in the same folder and do
src = awf.FrameInfo(src, &quot;Source&quot;)
folder = &quot;/path/to/settings_folder&quot;
out = awf.InterleaveDir(src, folder, PrintInfo=True, first=extract, repeat=True)
</code></pre>
<p>如果你使用<code>yuuno</code>，你可以使用下面的iPython魔法来获得 悬停在预览屏幕上，使预览在两个源之间切换
屏幕。</p>
<pre><code>%vspreview --diff
clip_A = core.ffms2.Source(&quot;settings/crf/17.0&quot;)
clip_A.set_output()
clip_B = core.ffms2.Source(&quot;settings/crf/17.5&quot;)
clip_B.set_output(1)
</code></pre>
<p>通常情况下，你会想先测试一下比特率。只要在几个不同的CRF下进行编码，并与源文件进行比较，找到与源文件无法区分的最高CRF值。
现在，对数值进行四舍五入，最好是向下，并切换到2-pass。对于标准测试，测试qcomp（间隔为0.05），大内部的aq强度的aq模式（例如，对于一个aq模式做测试，aq强度从0.6到1.0，间隔为0.2），aq强度（间隔为0.05），merange（32，48和64），psy-rd（间隔为0.05），ipratio/bratio（间隔为0.05，距离保持为0.10），然后deblock（间隔为1）。
如果你认为 mbtree 有所帮助（即你正在对动画进行编码），请在打开 mbtree 的情况下重做这个过程。你可能不会想怎么改变顺序，但肯定可以这样做。</p>
<p>对于x265，顺序应该是qcomp、aq-mode、aq-strength、psy-rd、psy-rdoq、ipratio和pbratio，然后是deblock。</p>
<p>如果你想要一点额外的效率，你可以在你最终决定的每个设置的数值周围用较小的间隔再次进行测试。建议在你已经对每个设置做了一次测试之后再做，因为它们确实对彼此都有轻微的影响。</p>
<p>一旦你完成了用2-pass的测试设置，就切换回CRF，并重复寻找最高透明CRF值的过程。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h4 id="dxva"><a class="header" href="#dxva">DXVA</a></h4>
<ul>
<li>
<p><code>--level 4.1</code></p>
</li>
<li>
<p><code>--vbv-bufsize 78125 --vbv-maxrate 62500</code> for DXVA (The old guide
used lower values to account for the possibility of writing the
encode to BD for playback, this is no longer a consideration as
other settings break this compatibility. The new values are the max
level 4.1 can do, if your device breaks because of this the encode
is not at fault, your device doesn't meet DXVA
spec).</p>
</li>
</ul>
<h2 id="general-settings"><a class="header" href="#general-settings">General settings</a></h2>
<ul>
<li>
<p><code>--b-adapt 2</code> uses the best algorithm (that x264 has) to decide how
B frames are placed.</p>
</li>
<li>
<p><code>--min-keyint</code> should typically be the frame rate of your video,
e.g. if you were encoding 23.976 fps content, then you use 24. This
is setting the minimum distance between I-frames.</p>
</li>
<li>
<p><code>--rc-lookahead 250</code> if using mbtree, 60 or higher else. This sets
how many frames ahead x264 can look, which is critical for mbtree.
You need lots of memory for this. (Personally I just leave this at
250 now as the impact on memory usage is 2 GB or so.) Definitely
lower this if you're encoding without mbtree and have a lot of
ReplaceFramesSimple calls in your script.</p>
</li>
<li>
<p><code>--me umh</code> is the lowest you should go. If your CPU is fast enough,
you might endure the slowdown from tesa. esa takes as much time as
tesa without any benefit, so if you want to slow down your encode to
try and catch more movement vectors, just use tesa, although the
increase is not necessarily always worth it. This isn't really a
setting you need to test, but on tough sources, you might squeeze
some more performance out of x264 if you use tesa.</p>
</li>
<li>
<p><code>--direct auto</code> will automatically choose the prediction mode
(spatial/temporal)</p>
</li>
<li>
<p><code>--subme 10</code> or <code>11</code> (personally I just set this to 11 the
difference in encode speed is within 3-4%)</p>
</li>
<li>
<p><code>--trellis 2</code></p>
</li>
<li>
<p><code>--no-dct-decimate</code> dct-decimate is a speed up that sacrifices
quality. Just leave it off, since your computers can likely handle
it.</p>
</li>
<li>
<p><code>--no-fast-pskip</code> Similar to the above.</p>
</li>
<li>
<p><code>--preset veryslow</code> or <code>placebo</code>, although the stuff we're changing
will make veryslow be placebo, anyway.</p>
</li>
</ul>
<h2 id="source-specific-settings"><a class="header" href="#source-specific-settings">Source-specific settings</a></h2>
<ul>
<li>
<p><code>--bitrate</code> / <code>--crf</code> Bitrate is in Kbps (Kilobits per second) and
CRF takes a float, where lower is better quality. This is the most
important setting you have; bitstarve an encode and it is guaranteed
to look like crap. Use too many bits and you've got bloat (and if
people wanted to download massive files, they would get a remux). Of
course, the bitrate need can vary drastically depending on the
source.</p>
</li>
<li>
<p><code>--deblock -3:-3</code> to <code>0:0</code>. For live action, most people just stick
with -3:-3. For anime, values between -3:-2 and 0:0 are common.
An explanation of the two params can be found <a href="https://forum.doom9.org/showpost.php?p=1692393&amp;postcount=32">HERE</a>.</p>
</li>
<li>
<p><code>--qcomp 0.6</code> (default) to <code>0.8</code> might prove useful. Don't set this
too high or too low, or the overall quality of your encode will
suffer. This setting has a heavy effect on mbtree. A higher qcomp
value will make mbtree weaker, hence something around 0.80 is
usually optimal for anime. <code>--qcomp 0</code> will cause a constant
bitrate, while <code>--qcomp 1</code> will cause a constant quantizer.</p>
</li>
<li>
<p><code>--aq-mode 1</code> to <code>3</code>: 1 distributes bits on a per frame basis, 2
tends to allocate more bits to the foreground and can distribute
bits in a small range of frames, 3 is a modified version of 2 that
attempts to allocate more bits to dark parts of the frames. The only
way to know what's best for sure it to test. Almost every source
ends up looking best with aq-mode 3, however.</p>
</li>
<li>
<p><code>--aq-strength 0.5</code> to <code>1.3</code> are worth trying. Higher values might
help with blocking. Lower values will tend to allocate more bits to
the foreground, similar to aq-mode 2. Lower values allocate more
bits to flat areas, while higher values allocate more to
&quot;detailed&quot; areas. Note that x264 considers grain and noise as
detail, so you can think of this setting as the ratio of bits
allocated to edges vs. bits allocated to grain. You're usually going
to want to have a higher qcomp for a lower aq-strength and vice
versa. With mode 3, you're usually going to end up somewhere around
the 0.7-0.8 range for modern film, for which a qcomp of 0.6-0.7
often works best.</p>
</li>
<li>
<p><code>--merange 24</code> (the lowest that should ever be used) to <code>64</code>,
setting this too high can hurt (more than 128), 32 or 48 will be
fine for most encodes. In general, 32-48 for 1080p and 32 for 720p
(when using umh) for movies with lots of motion this can help (e.g.
action movies). Talking heads can get away with low values like 24.
The impact on encode speed is noticeable but not horrible. I prefer
to use 48 for 1080p and 32 for 720p when using umh or 32 for 1080p
and 32 for 720p when using tesa.</p>
</li>
<li>
<p><code>--no-mbtree</code> I highly recommend testing with both mbtree enabled
and disabled, as generally it will result in two very different
encodes. mbtree basically attempts to degrade the quality of blocks
rather than frames, so that only the unimportant parts of a frame
get less bits. To do this, it needs to know how often a block is
referenced later, which is why <code>--rc-lookahead</code> should be set
to 250. Useful for things with static backgrounds, like anime. Or
for things where you've used a high qcomp (.75 or above) and mbtree
will have a lowered impact. When testing whether this is a decent
option, you'll likely have to retest every setting, especially
qcomp, psy-rd, and ipratio.</p>
</li>
<li>
<p><code>--ipratio 1.15</code> to <code>1.40</code>, with 1.30 usually being the go-to
option. This is the bitrate allocation ratio between I and P frames.</p>
</li>
<li>
<p><code>--pbratio 1.05</code> to <code>1.30</code>, with 1.20 being the usual go-to. This is
the bitrate allocation ratio between P and B frames. This value
should always be around 0.10 lower than --ipratio, so lower it
while testing ipratio. If you're using mbtree, this setting will
have no affect, as mbtree determines it itself.</p>
</li>
<li>
<p><code>--psy-rd 0.40:0 to 1.15:0</code>: 0.95:0 to 1.15:0 for live action. The
first number is psy-rd strength, second is psy-trellis strength.
This tries to keep x264 from making things blurry and instead keep
the complexity. For anime, between 0.40 and 1.00:0.00 is the usual
range. Psy-trellis usually introduces a lot of ringing, but can help
with maintaining dither. You can try values between 0.00 and 0.15
for live action and try values up to 0.50 for anime, although you'll
usually get better results if you raise your aq-strength instead.</p>
</li>
<li>
<p><code>--bframes 6</code> to <code>16</code>, This is setting the maximum amount of
consecutive P frames that can be replaced with B frames. Test with
16 for your first test run, and set according to the x264 log:</p>
<p><code>x264 [info]: consecutive B-frames: 1.0% 0.0% 0.0% 0.0% 14.9% 23.8% 13.9% 15.8% 8.9% 9.9% 0.0% 11.9% 0.0% 0.0% 0.0% 0.0% 0.0%</code></p>
<p>Start counting with the first percentage as 0 and choose the highest
number with more than 1%, which is 11 in this example.</p>
<p>Or just leave this at 16, as allowing more bframes will not harm
your encode and will aid in compression; the impact on speed isn't that
enormous.</p>
</li>
<li>
<p><code>--ref 16</code> if you don't care about hardware compatibility, else just
leave this option out and x264 will determine the correct number
according to your level. This sets the number of previous frames
each P frame can use as references. The impact on performance is
quite high, but it's worth it most of the time.</p>
</li>
<li>
<p><code>--zones</code> is quite useful for debanding and blocking, as these areas
require a larger bitrate to maintain transparency. The syntax is<br />
<code>--zones 0,100,crf=10/101,200,crf=15</code> or<br />
<code>--zones 0,100,b=5/101,200,b=10</code> with <code>b</code> being a bitrate multiplier
in this case. You can also use this for areas that don't get enough
bits allocated. Especially common areas are darker scenes or scenes
with lots of reds. Fades can also suffer from bitstarving and
require zoning. One can also lower the bitrate during credits to
save that little something.</p>
</li>
<li>
<p><code>--output-depth 8</code> or <code>10</code> depending on what you're encoding in.</p>
</li>
<li>
<p><code>--output-csp i444</code> if you're encoding 4:4:4, else leave this out.</p>
</li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><p>The documentation here is very good, so I'll just go over recommended
values:</p>
<h2 id="source-independent-settings"><a class="header" href="#source-independent-settings">Source-independent settings</a></h2>
<ul>
<li>
<p><code>--preset veryslow</code> or <code>slower</code></p>
</li>
<li>
<p><code>--no-rect</code> for slower computers. There's a slight chance it'll
prove useful, but it probably isn't worth it.</p>
</li>
<li>
<p><code>--no-amp</code> is similar to <code>rect</code>, although it seems to be slightly
more useful.</p>
</li>
<li>
<p><code>--no-open-gop</code></p>
</li>
<li>
<p><code>--no-cutree</code> since this seems to be a poor implementation of
<code>mbtree</code>.</p>
</li>
<li>
<p><code>--no-rskip</code> <code>rskip</code> is a speed up that gives up some quality, so
it's worth considering with bad CPUs.</p>
</li>
<li>
<p><code>--ctu 64</code></p>
</li>
<li>
<p><code>--min-cu-size 8</code></p>
</li>
<li>
<p><code>--rdoq-level 2</code></p>
</li>
<li>
<p><code>--max-merge 5</code></p>
</li>
<li>
<p><code>--rc-lookahead 60</code> although it's irrelevant as long as it's larger
than min-keyint</p>
</li>
<li>
<p><code>--ref 6</code> for good CPUs, something like <code>4</code> for worse ones.</p>
</li>
<li>
<p><code>--bframes 16</code> or whatever your final bframes log output says.</p>
</li>
<li>
<p><code>--rd 3</code> or <code>4</code> (they're currently the same).  If you can endure the slowdown, you can use <code>6</code>, too, which allows you to test <code>--rd-refine</code>.</p>
</li>
<li>
<p><code>--subme 5</code>. You can also change this to <code>7</code>, but this is known to
sharpen.</p>
</li>
<li>
<p><code>--merange 57</code> just don't go below <code>32</code> and you should be fine.</p>
</li>
<li>
<p><code>--high-tier</code></p>
</li>
<li>
<p><code>--range limited</code></p>
</li>
<li>
<p><code>--aud</code></p>
</li>
<li>
<p><code>--repeat-headers</code></p>
</li>
</ul>
<h2 id="source-dependent-settings"><a class="header" href="#source-dependent-settings">Source-dependent settings</a></h2>
<ul>
<li>
<p><code>--output-depth 10</code> for 10-bit output.</p>
</li>
<li>
<p><code>--input-depth 10</code> for 10-bit input.</p>
</li>
<li>
<p><code>--colorprim 9</code> for HDR, <code>1</code> for SDR.</p>
</li>
<li>
<p><code>--colormatrix 9</code> for HDR, <code>1</code> for SDR.</p>
</li>
<li>
<p><code>--transfer 16</code> for HDR, <code>1</code> for SDR.</p>
</li>
<li>
<p><code>--hdr10</code> for HDR.</p>
</li>
<li>
<p><code>--hdr10-opt</code> for 4:2:0 HDR, <code>--no-hdr10-opt</code> for 4:4:4 HDR and SDR.</p>
</li>
<li>
<p><code>--dhdr10-info /path/to/metadata.json</code> for HDR10+ content with metadata extracted using <a href="https://github.com/quietvoid/hdr10plus_parser">hdr10plus_parser</a>.</p>
</li>
</ul>
<ul>
<li>
<p><code>--dolby-vision-profile 8.1</code> specified Dolby Vision profile. x265 can encode only to profiles <code>5</code>, <code>8.1</code>, and <code>8.2</code></p>
</li>
<li>
<p><code>--dolby-vision-rpu /path/to/rpu.bin</code> for Dolby Vision metadata extracted using <a href="https://github.com/quietvoid/dovi_tool">dovi_tool</a>.</p>
</li>
<li>
<p><code>--master-display &quot;G(8500,39850)B(6550,2300)R(35400,14600)WP(15635,16450)L(10000000,20)&quot;</code>
for BT.2020 or<br />
<code>G(13250,34500)B(7500,3000)R(34000,16000)WP(15635,16450)L(10000000,1)</code>
for Display P3 mastering display color primaries with the values for
L coming from your source's MediaInfo for mastering display
luminance.</p>
<p>For example, if your source MediaInfo reads:</p>
<pre><code>Mastering display color primaries : BT.2020
Mastering display luminance : min: 0.0000 cd/m2, max: 1000 cd/m2
Maximum Content Light Level : 711 cd/m2
Maximum Frame-Average Light Level : 617 cd/m2
</code></pre>
<p>This means you set <code>&quot;G(8500,39850)B(6550,2300)R(35400,14600)WP(15635,16450)L(10000000,0)&quot;</code></p>
</li>
<li>
<p><code>--max-cll &quot;711,617&quot;</code> from your source's MediaInfo for maximum
content light level and maximum frame-average light level.
The values here are from the above example.</p>
</li>
<li>
<p><code>--cbqpoffs</code> and <code>--crqpoffs</code> should usually be between -3 and 0 for 4:2:0.
For 4:4:4, set this to something between 3 and 6.
This sets an offset between the bitrate applied to the luma and the
chroma planes.</p>
</li>
<li>
<p><code>--qcomp</code> between <code>0.60</code> and <code>0.80</code>.</p>
</li>
<li>
<p><code>--aq-mode 4</code>, <code>3</code>, <code>2</code>, <code>1</code>, or <code>--hevc-aq</code> with <code>4</code> and <code>3</code>
usually being the two best options. These do the following</p>
<ol>
<li>
<p>Standard adaptive quantization, simply add more bits to complex
blocks.</p>
</li>
<li>
<p>Adaptive quantization with auto-variance.</p>
</li>
<li>
<p>Adaptive quantization with auto-variance and bias to dark scene.</p>
</li>
<li>
<p>Adaptive quantization with auto-variance and better edge
preservation.</p>
</li>
<li>
<p><code>hevc-aq</code> &quot;scales the quantization step size according to the
spatial activity of one coding unit relative to frame average
spatial activity. This AQ method utilizes the minimum variance
of sub-unit in each coding unit to represent the coding unit's
spatial complexity.&quot; Like most of the x265 documentation, this
sounds a lot fancier than it is. Don't enable with other modes
turned on.</p>
</li>
</ol>
</li>
<li>
<p><code>--aq-strength</code> between <code>0.80</code> and <code>1.40</code> for AQ modes 1-3 or <code>0.50</code> and <code>1.00</code> for AQ mode 4.</p>
</li>
<li>
<p><code>--deblock -4:-4</code> to <code>0:0</code>, as with x264. You can default <code>-3:-3</code>
with live action.</p>
</li>
<li>
<p><code>--ipratio</code> and <code>--pbratio</code> same as x264 again.</p>
</li>
<li>
<p><code>--psy-rd 0.80</code> to <code>2.00</code>, similar-ish effect to x264.  Values are generally higher than with x264, though.</p>
</li>
<li>
<p><code>--psy-rdoq</code> anything from <code>0.00</code> to <code>2.00</code> usually.</p>
</li>
<li>
<p><code>--no-sao</code> is usually best, but if your encode suffers from a lot of ringing, turn SAO back on.  SAO does tend to blur quite heavily.</p>
</li>
<li>
<p><code>--no-strong-intra-smoothing</code> on sharp/grainy content, you can leave
this on for blurry content, as it's an additional blur that'll help
prevent banding.</p>
</li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="taking-screenshots"><a class="header" href="#taking-screenshots">Taking Screenshots</a></h1>
<p>在VapourSynth中拍摄简单的屏幕截图是非常容易的。如果你使用的是预览器，你可以用它来代替，但知道如何直接通过VapourSynth进行截图可能还是很有用的。</p>
<p>我们推荐使用 <code>awsmfunc.ScreenGen</code>.
它有两个优势:</p>
<ol>
<li>保存了帧数信息，可以很容易地再次参考这些，例如，如果你想重新做你的截图。</li>
<li>它为你处理适当的转换和压缩，而有些预览器（如VSEdit）可能不是这样。</li>
</ol>
<p>要使用 &quot;ScreenGen&quot;，创建一个你想要截图的新文件夹，例如 &quot;Screenshots&quot;，和一个名为 &quot;screen.txt &quot;的文件，其中包含你想要截图的帧数。, 例如：</p>
<pre><code>26765
76960
82945
92742
127245
</code></pre>
<p>然后，在你的VapourSynth脚本的底部，写上</p>
<pre><code>awf.ScreenGen(src, &quot;Screenshots&quot;, &quot;a&quot;)
</code></pre>
<p>`a'是放在帧号后面的东西。这对于保持组织性和对截图进行分类是很有用的，同时也可以防止不必要的截图被覆盖。</p>
<p>现在，在命令行中运行你的脚本（或在预览器中重新加载）。</p>
<pre><code class="language-sh">python vapoursynth_script.vpy
</code></pre>
<p>完成!
你的截图现在应该在给定的文件夹中。</p>
<h1 id="comparing-source-vs-encode"><a class="header" href="#comparing-source-vs-encode">Comparing Source vs. Encode</a></h1>
<p>将源码与你的编码进行比较，可以让潜在的下载者轻松判断你的编码质量。
当采取这些时，重要的是包括你要比较的框架类型，例如比较两个<code>I'框架将导致极其有利的结果。 你可以使用</code>awsmfunc.FrameInfo`来做这个。</p>
<pre><code class="language-py">src = awf.FrameInfo(src, &quot;Source&quot;)
encode = awf.FrameInfo(encode, &quot;Encode&quot;)
</code></pre>
<p>如果你想在你的预览器中比较这些，建议将它们交错排列。</p>
<pre><code class="language-py">out = core.std.Interleave([src, encode])
</code></pre>
<p>然而，如果你是用<code>ScreenGen</code>进行截图，不这样做更容易，只需运行两个<code>ScreenGen</code>调用。</p>
<pre><code class="language-py">src = awf.FrameInfo(src, &quot;Source&quot;)
awf.ScreenGen(src, &quot;Screenshots&quot;, &quot;a&quot;)
encode = awf.FrameInfo(encode, &quot;Encode&quot;)
awf.ScreenGen(encode, &quot;Screenshots&quot;, &quot;b&quot;)
</code></pre>
<p>注意，在编码的<code>ScreenGen</code>中，<code>&quot;a&quot;</code>被替换成了<code>&quot;b&quot;</code>。
这将允许你按名字对文件夹进行排序，并且每张源截图后面都有一个编码截图，使上传更容易。</p>
<h3 id="hdr-comparisons"><a class="header" href="#hdr-comparisons">HDR comparisons</a></h3>
<p>对于比较HDR源和HDR编码，建议使用色调图。
这个过程是破坏性的，但你仍然能够分辨出哪些地方被扭曲了，哪些地方被平滑了等等。</p>
<p>为此推荐的函数是<code>awsmfunc.DynamicTonemap</code>:</p>
<pre><code class="language-py">src = awf.DynamicTonemap(src, src_fmt=False, libplacebo=False)
encode = awf.DynamicTonemap(encode, src_fmt=False, libplacebo=False)
</code></pre>
<p>注意，我们在这里禁用了<code>src_fmt</code>和<code>libplacebo</code>。
将前者设置为<code>True</code>会输出10位4:2:0，这是次优的，因为屏幕截图通常是以8位RGB（没有色度子采样）呈现。
后者被推荐用于比较，因为使用libplacebo会使你的色调图在亮度上更有可能不同，使比较更加困难。</p>
<h2 id="choosing-frames"><a class="header" href="#choosing-frames">Choosing frames</a></h2>
<p>在进行截图时，重要的是不要让你的编码看起来有欺骗性的透明。
要做到这一点，你需要确保你截图的是适当的框架类型，以及内容上不同的框架。</p>
<p>幸运的是，这里没有太多需要记住的东西:</p>
<ul>
<li>你的编码的截图应该<em>永远</em>是<em>B</em>类型帧。</li>
<li>你的源的截图<em>不应该</em>是<em>I</em>类型帧。</li>
<li>你的比较应该包括黑暗场景、明亮场景、特写镜头、远景镜头、静态场景、高度动作场景，以及你在这两者之间的任何内容。</li>
</ul>
<h1 id="comparing-different-sources"><a class="header" href="#comparing-different-sources">Comparing Different Sources</a></h1>
<p>当比较不同的来源时，你应该进行类似于比较来源与编码的工作。
然而，你可能会遇到不同的裁剪、分辨率或色调，所有这些都会妨碍比较的进行。</p>
<p>对于不同的裁剪，只需将边框加回去:</p>
<pre><code class="language-py">src_b = src_b.std.AddBorders(left=X, right=Y, top=Z, bottom=A)
</code></pre>
<p>如果这样做会导致图像内容的偏移，你应该调整大小为4:4:4，这样你就可以添加不均匀的边框。
例如，如果你想在顶部和底部添加1像素高的黑条:</p>
<pre><code class="language-py">src_b = src_b.resize.Spline36(format=vs.YUV444P8, dither_type=&quot;error_diffusion&quot;)
src_b = src_b.std.AddBorders(top=1, bottom=1)
</code></pre>
<p>对于不同的分辨率，建议使用一个简单的花键调整大小:</p>
<pre><code class="language-py">src_b = src_b.resize.Spline36(src_a.width, src_a.height, dither_type=&quot;error_diffusion&quot;)
</code></pre>
<p>如果一个源是HDR，另一个是SDR，你可以使用<code>awsmfunc.DynamicTonemap</code>:</p>
<pre><code class="language-py">src_b = awf.DynamicTonemap(src_b, src_fmt=False)
</code></pre>
<p>关于不同的色调，请参考<a href="encoding/../filtering/detinting.html">色调篇</a>.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><p>如果你的去带片段与没有带子的部分相比颗粒很小，你应该考虑使用一个单独的函数来添加匹配的颗粒，这样场景就更容易融合在一起。如果有很多颗粒，你可能要考虑<code>adptvgrnMod</code>、<code>adaptive_grain</code>或<code>GrainFactory3</code>；对于不那么明显的颗粒，或者只是对于通常会有很少颗粒的明亮场景，你也可以使用<code>grain.Add</code>。颗粒器的主题将在后面的<a href="appendix/../filtering/graining.html">粒化部分</a>中进一步阐述。</p>
<p>下面是Mirai的一个例子:</p>
<p align="center">
<img src='Pictures/banding_graining_before.png' onmouseover="this.src='Pictures/banding_graining_after.png';" onmouseout="this.src='Pictures/banding_graining_before.png';"/>
</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="black--white-clips-working-in-gray"><a class="header" href="#black--white-clips-working-in-gray">Black &amp; White Clips: Working in GRAY</a></h1>
<p>由于YUV格式将luma保存在一个单独的平面内，在处理黑白电影时，我们会变得更加轻松，因为我们可以提取luma平面并只在该平面上工作。</p>
<pre><code class="language-py">y = src.std.ShufflePlanes(0, vs.GRAY)
</code></pre>
<p><code>vsutil</code>中的<code>get_y</code>函数做了同样的事情。
有了我们的<code>y</code>片段，我们可以执行没有mod2限制的功能。
例如，我们可以进行奇数裁剪:</p>
<pre><code class="language-py">crop = y.std.Crop(left=1)
</code></pre>
<p>此外，由于滤镜只应用在一个平面上，这可以加快我们的脚本速度。
我们也不必担心像<code>f3kdb</code>这样的滤镜会以不需要的方式改变我们的色度平面，例如使它们产生颗粒。</p>
<p>然而，当我们完成我们的剪辑后，我们通常想把剪辑导出为YUV。
这里有两个选项:</p>
<h3 id="1-使用假色度"><a class="header" href="#1-使用假色度">1. 使用假色度</a></h3>
<p>使用假的色度是快速和简单的，其优点是在源中任何意外的色度偏移（例如色度颗粒）将被删除。
所有这一切都需要恒定的色度（意味着没有色调变化）和mod2 luma。</p>
<p>最简单的选项是<code>u = v = 128</code>（8位）:</p>
<pre><code class="language-py">out = y.resize.Point(format=vs.YUV420P8)
</code></pre>
<p>如果你有一个不均匀的luma，只需用<code>awsmfunc.fb</code>来填充它。
假设你想对左边进行填充:</p>
<pre><code class="language-py">y = core.std.StackHorizontal([y.std.BlankClip(width=1), y])
y = awf.fb(y, left=1)
out = y.resize.Point(format=vs.YUV420P8)
</code></pre>
<p>另外，如果你的源的色度不是中性灰色，可以使用<code>std.BlankClip</code>:</p>
<pre><code class="language-py">blank = y.std.BlankClip(color=[0, 132, 124])
out = core.std.ShufflePlanes([y, blank], [0, 1, 2], vs.YUV)
</code></pre>
<h3 id="2-使用原始色度必要时调整大小"><a class="header" href="#2-使用原始色度必要时调整大小">2. 使用原始色度（必要时调整大小）</a></h3>
<p>这样做的好处是，如果有实际重要的色度信息（例如轻微的棕褐色色调），这将被保留下来。
只要在你的素材上使用<code>ShufflePlanes</code>就可以了:</p>
<pre><code class="language-py">out = core.std.ShufflePlanes([y, src], [0, 1, 2], vs.YUV)
</code></pre>
<p>然而，如果你已经调整了尺寸或裁剪，这就变得有点困难了。
你可能必须适当地移动或调整色度（见<a href="appendix/.../filtering/chroma_rs.html">色度重采样章节</a>的解释）。</p>
<p>如果你已经裁剪了，就提取并相应地移位。我们将使用<code>vsutil</code>的<code>split</code>和<code>join</code>来提取和合并平面:</p>
<pre><code class="language-py">y, u, v = split(src)
crop_left = 1
y = y.std.Crop(left=crop_left)
u = u.resize.Spline36(src_left=crop_left / 2)
v = v.resize.Spline36(src_left=crop_left / 2)
out = join([y, u, v])
</code></pre>
<p>如果你已经调整了尺寸，你需要转移和调整色度的大小:</p>
<pre><code class="language-py">y, u, v = split(src)
w, h = 1280, 720
y = y.resize.Spline36(w, h)
u = u.resize.Spline36(w / 2, h / 2, src_left=.25 - .25 * src.width / w)
v = v.resize.Spline36(w / 2, h / 2, src_left=.25 - .25 * src.width / w)
out = join([y, u, v])
</code></pre>
<p>结合裁剪和移位，据此我们垫高裁剪并使用<code>awsmfunc.fb</code>来创建一个假线:</p>
<pre><code class="language-py">y, u, v = split(src)
w, h = 1280, 720
crop_left, crop_bottom = 1, 1

y = y.std.Crop(left=crop_left, bottom=crop_bottom)
y = y.resize.Spline36(w - 1, h - 1)
y = core.std.StackHorizontal([y.std.BlankClip(width=1), y])
y = awf.fb(left=1)

u = u.resize.Spline36(w / 2, h / 2, src_left=crop_left / 2 + (.25 - .25 * src.width / w), src_height=u.height - crop_bottom / 2)

v = v.resize.Spline36(w / 2, h / 2, src_left=crop_left / 2 + (.25 - .25 * src.width / w), src_height=u.height - crop_bottom / 2)

out = join([y, u, v])
</code></pre>
<p>如果你不明白这里到底发生了什么，遇到这样的情况，请向更有经验的人求助。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="a"><a class="header" href="#a">A</a></h1>
<ul>
<li><a href="https://git.kageru.moe/kageru/adaptivegrain">adaptivegrain</a></li>
<li><a href="https://github.com/HomeOfVapourSynthEvolution/VapourSynth-AddGrain">AddGrain</a></li>
<li><a href="https://gitlab.com/Ututu/adptvgrnmod">adptvgrnMod</a></li>
<li><a href="https://git.concertos.live/AHD/awsmfunc">awsmfunc</a></li>
</ul>
<h1 id="b"><a class="header" href="#b">B</a></h1>
<ul>
<li><a href="https://gitlab.com/snippets/1834089">black_detect</a></li>
<li><a href="https://gist.github.com/blaze077/ac896645913938591b45e7d65932b136">blazefunc</a></li>
</ul>
<h1 id="c"><a class="header" href="#c">C</a></h1>
<ul>
<li><a href="https://valeyard.net/2017/03/drdres-color-matching-tool-v1-2.php">ColorMatch</a></li>
<li><a href="https://gitlab.com/Ututu/VS-ContinuityFixer">ContinuityFixer</a></li>
</ul>
<h1 id="d"><a class="header" href="#d">D</a></h1>
<ul>
<li><a href="https://pastebin.com/SHQZjVJ5">debandmask</a></li>
</ul>
<h1 id="e"><a class="header" href="#e">E</a></h1>
<ul>
<li><a href="https://github.com/sekrit-twc/EdgeFixer">EdgeFixer</a></li>
<li><a href="https://gist.github.com/4re/342624c9e1a144a696c6">edi_rpow2</a></li>
</ul>
<h1 id="f"><a class="header" href="#f">F</a></h1>
<ul>
<li><a href="https://gist.github.com/Frechdachs/9d9b50d050fa11e438eae5d967296e0e">fag3kdb</a></li>
<li><a href="https://github.com/dubhater/vapoursynth-fillborders">FillBorders</a></li>
<li><a href="https://github.com/EleonoreMizo/fmtconv">fmtconv</a></li>
<li><a href="https://github.com/igv/FSRCNN-TensorFlow">FSRCNNX</a></li>
<li><a href="https://github.com/Irrational-Encoding-Wizardry/fvsfunc">fvsfunc</a></li>
</ul>
<h1 id="g"><a class="header" href="#g">G</a></h1>
<ul>
<li><a href="https://github.com/Infiziert90/getnative">getnative</a></li>
</ul>
<h1 id="k"><a class="header" href="#k">K</a></h1>
<ul>
<li><a href="https://github.com/Irrational-Encoding-Wizardry/kagefunc">kagefunc</a></li>
<li><a href="https://gist.github.com/igv/a015fc885d5c22e6891820ad89555637">KrigBilateral</a></li>
</ul>
<h1 id="l"><a class="header" href="#l">L</a></h1>
<ul>
<li><a href="https://github.com/haasn/libplacebo">libplacebo</a></li>
<li><a href="https://github.com/Irrational-Encoding-Wizardry/lvsfunc">lvsfunc</a></li>
</ul>
<h1 id="m"><a class="header" href="#m">M</a></h1>
<ul>
<li><a href="https://github.com/HomeOfVapourSynthEvolution/mvsfunc">mvsfunc</a></li>
<li><a href="https://github.com/WolframRhodium/muvsfunc">muvsfunc</a></li>
</ul>
<h1 id="n"><a class="header" href="#n">N</a></h1>
<ul>
<li><a href="https://github.com/HomeOfAviSynthPlusEvolution/neo_f3kdb/">neo_f3kdb</a></li>
</ul>
<h1 id="r"><a class="header" href="#r">R</a></h1>
<ul>
<li><a href="https://gitlab.com/Ututu/rekt">rekt</a></li>
<li><a href="https://github.com/Irrational-Encoding-Wizardry/RgToolsVS">RgToolsVS</a></li>
</ul>
<h1 id="t"><a class="header" href="#t">T</a></h1>
<ul>
<li><a href="https://github.com/sekrit-twc/timecube">timecube</a></li>
</ul>
<h1 id="v"><a class="header" href="#v">V</a></h1>
<ul>
<li><a href="https://github.com/HomeOfVapourSynthEvolution/VapourSynth-Bilateral">VapourSynth bilateral</a></li>
<li><a href="https://www.vapoursynth.com/doc/functions.html">VapourSynth Functions</a></li>
<li><a href="https://github.com/Lypheo/vs-placebo">vs-placebo</a></li>
<li><a href="https://github.com/HomeOfVapourSynthEvolution/vsTAAmbk">vsTAAmbk</a></li>
<li><a href="https://github.com/Irrational-Encoding-Wizardry/vsutil">vsutil</a></li>
</ul>
<h1 id="z"><a class="header" href="#z">Z</a></h1>
<ul>
<li><a href="https://github.com/kgrabs/zzfunc">zzfunc</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
